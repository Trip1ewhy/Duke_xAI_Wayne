{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Trip1ewhy/Duke_xAI_Wayne/blob/main/Wayne_Finetune_Llama3_with_LLaMA_Factory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetune Llama-3 with LLaMA Factory\n",
        "\n",
        "Please use a **free** Tesla T4 Colab GPU to run this!\n",
        "\n",
        "Project homepage: https://github.com/hiyouga/LLaMA-Factory"
      ],
      "metadata": {
        "id": "1oHFCsV0z-Jw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies"
      ],
      "metadata": {
        "id": "lr7rB3szzhtx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giM74oK1rRIH",
        "outputId": "b025bb40-542f-4366-b151-8a4b1f9e7e26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 321, done.\u001b[K\n",
            "remote: Counting objects: 100% (321/321), done.\u001b[K\n",
            "remote: Compressing objects: 100% (265/265), done.\u001b[K\n",
            "remote: Total 321 (delta 76), reused 149 (delta 43), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (321/321), 8.98 MiB | 4.41 MiB/s, done.\n",
            "Resolving deltas: 100% (76/76), done.\n",
            "/content/LLaMA-Factory\n",
            "\u001b[0m\u001b[01;34massets\u001b[0m/       \u001b[01;34mdocker\u001b[0m/      LICENSE      pyproject.toml  requirements.txt  \u001b[01;34msrc\u001b[0m/\n",
            "CITATION.cff  \u001b[01;34mevaluation\u001b[0m/  Makefile     README.md       \u001b[01;34mscripts\u001b[0m/          \u001b[01;34mtests\u001b[0m/\n",
            "\u001b[01;34mdata\u001b[0m/         \u001b[01;34mexamples\u001b[0m/    MANIFEST.in  README_zh.md    setup.py\n",
            "Collecting torch==2.3.1\n",
            "  Downloading torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting torchvision==0.18.1\n",
            "  Downloading torchvision-0.18.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting torchaudio==2.3.1\n",
            "  Downloading torchaudio-2.3.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.1)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.1)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.1)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.1)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.1)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.1)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.1)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.1)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.1)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.1)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.3.1 (from torch==2.3.1)\n",
            "  Downloading triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.18.1) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.18.1) (11.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.1) (12.6.77)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.1) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.1) (1.3.0)\n",
            "Downloading torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.18.1-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m112.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.3.1-cp310-cp310-manylinux1_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m211.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ],
      "source": [
        "%cd /content/\n",
        "%rm -rf LLaMA-Factory\n",
        "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
        "%cd LLaMA-Factory\n",
        "%ls\n",
        "!pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1\n",
        "!pip uninstall -y jax\n",
        "!pip install -e .[torch,bitsandbytes,liger-kernel]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check GPU environment"
      ],
      "metadata": {
        "id": "H9RXn_YQnn9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "try:\n",
        "  assert torch.cuda.is_available() is True\n",
        "except AssertionError:\n",
        "  print(\"Please set up a GPU before using LLaMA Factory: https://medium.com/mlearning-ai/training-yolov4-on-google-colab-316f8fff99c6\")"
      ],
      "metadata": {
        "id": "ZkN-ktlsnrdU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Update Identity Dataset"
      ],
      "metadata": {
        "id": "TeYs5Lz-QJYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "NAME = \"Llama-3\"\n",
        "AUTHOR = \"LLaMA Factory\"\n",
        "\n",
        "with open(\"data/identity.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "  dataset = json.load(f)\n",
        "\n",
        "for sample in dataset:\n",
        "  sample[\"output\"] = sample[\"output\"].replace(\"{{\"+ \"name\" + \"}}\", NAME).replace(\"{{\"+ \"author\" + \"}}\", AUTHOR)\n",
        "\n",
        "with open(\"data/identity.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "  json.dump(dataset, f, indent=2, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "ap_fvMBsQHJc",
        "outputId": "8e475758-3318-4f3c-911b-52845531f720",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Update\n",
        "Add the following codes to the dataset_info.json in /data\n",
        "\n",
        "\"moral_training_data\": {\n",
        "    \"file_name\": \"moral_training_data.json\"\n",
        "  },\n",
        "\n",
        "And upload moral_training_data.json to /data\n"
      ],
      "metadata": {
        "id": "oOMjYW4a5l3t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Login in huggingface\n",
        "Using llama3.1-8B"
      ],
      "metadata": {
        "id": "whh-rECb6hUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "vr-YYgPj6guK",
        "outputId": "c9276e40-4985-4f53-d3b4-0e650411d3e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `Wayne_duke_xAI` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `Wayne_duke_xAI`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune model via LLaMA Board"
      ],
      "metadata": {
        "id": "2QiXcvdzzW3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/LLaMA-Factory/\n",
        "!GRADIO_SHARE=1 llamafactory-cli webui"
      ],
      "metadata": {
        "id": "YLsdS6V5yUMy",
        "outputId": "42200ee5-f6f5-4e52-ff20-10e2e8598680",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/LLaMA-Factory\n",
            "2024-12-04 23:07:05.579744: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-04 23:07:05.597733: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-04 23:07:05.620373: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-04 23:07:05.627293: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-04 23:07:05.643806: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-04 23:07:06.871822: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Running on local URL:  http://0.0.0.0:7860\n",
            "Running on public URL: https://181c974d685cf16ce2.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "2024-12-04 23:08:15.674944: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-04 23:08:15.691330: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-04 23:08:15.712314: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-04 23:08:15.718665: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-04 23:08:15.733527: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-04 23:08:16.936632: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[INFO|2024-12-04 23:08:22] llamafactory.hparams.parser:355 >> Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.bfloat16\n",
            "config.json: 100% 826/826 [00:00<00:00, 5.22MB/s]\n",
            "[INFO|configuration_utils.py:679] 2024-12-04 23:08:22,201 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/config.json\n",
            "[INFO|configuration_utils.py:746] 2024-12-04 23:08:22,203 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Meta-Llama-3.1-8B\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 8.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "tokenizer_config.json: 100% 50.5k/50.5k [00:00<00:00, 4.88MB/s]\n",
            "tokenizer.json: 100% 9.09M/9.09M [00:00<00:00, 37.4MB/s]\n",
            "special_tokens_map.json: 100% 73.0/73.0 [00:00<00:00, 533kB/s]\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:08:23,139 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:08:23,139 >> loading file tokenizer.model from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:08:23,139 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:08:23,140 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:08:23,140 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2475] 2024-12-04 23:08:23,686 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:679] 2024-12-04 23:08:24,095 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/config.json\n",
            "[INFO|configuration_utils.py:746] 2024-12-04 23:08:24,096 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Meta-Llama-3.1-8B\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 8.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:08:24,201 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:08:24,201 >> loading file tokenizer.model from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:08:24,201 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:08:24,201 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:08:24,201 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2475] 2024-12-04 23:08:24,707 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|2024-12-04 23:08:24] llamafactory.data.template:157 >> Add pad token: <|end_of_text|>\n",
            "[INFO|2024-12-04 23:08:24] llamafactory.data.loader:157 >> Loading dataset moral_training_data.json...\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/llamafactory-cli\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/cli.py\", line 112, in main\n",
            "    run_exp()\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 50, in run_exp\n",
            "    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/train/sft/workflow.py\", line 51, in run_sft\n",
            "    dataset_module = get_dataset(template, model_args, data_args, training_args, stage=\"sft\", **tokenizer_module)\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/data/loader.py\", line 266, in get_dataset\n",
            "    dataset = _get_merged_dataset(data_args.dataset, model_args, data_args, training_args, stage)\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/data/loader.py\", line 172, in _get_merged_dataset\n",
            "    datasets.append(_load_single_dataset(dataset_attr, model_args, data_args, training_args))\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/data/loader.py\", line 75, in _load_single_dataset\n",
            "    raise ValueError(f\"File {local_path} not found.\")\n",
            "ValueError: File data/moral_training_data.json not found.\n",
            "2024-12-04 23:08:46.384117: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-04 23:08:46.400452: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-04 23:08:46.421476: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-04 23:08:46.428160: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-04 23:08:46.443092: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-04 23:08:47.667804: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[INFO|2024-12-04 23:08:52] llamafactory.hparams.parser:355 >> Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.bfloat16\n",
            "[INFO|configuration_utils.py:679] 2024-12-04 23:08:52,917 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/config.json\n",
            "[INFO|configuration_utils.py:746] 2024-12-04 23:08:52,918 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Meta-Llama-3.1-8B\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 8.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:08:53,093 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:08:53,093 >> loading file tokenizer.model from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:08:53,094 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:08:53,094 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:08:53,094 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2475] 2024-12-04 23:08:53,614 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:679] 2024-12-04 23:08:54,237 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/config.json\n",
            "[INFO|configuration_utils.py:746] 2024-12-04 23:08:54,238 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Meta-Llama-3.1-8B\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 8.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:08:54,396 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:08:54,396 >> loading file tokenizer.model from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:08:54,396 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:08:54,396 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:08:54,396 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2475] 2024-12-04 23:08:54,917 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|2024-12-04 23:08:54] llamafactory.data.template:157 >> Add pad token: <|end_of_text|>\n",
            "[INFO|2024-12-04 23:08:54] llamafactory.data.loader:157 >> Loading dataset moral_training_data.json...\n",
            "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
            "Generating train split: 201 examples [00:00, 3512.70 examples/s]\n",
            "Converting format of dataset (num_proc=16): 100% 201/201 [00:00<00:00, 885.02 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16): 100% 201/201 [00:04<00:00, 42.58 examples/s]\n",
            "training example:\n",
            "input_ids:\n",
            "[35075, 25, 1472, 527, 459, 6335, 304, 279, 2115, 315, 3823, 45651, 323, 7296, 13, 1472, 690, 387, 4691, 264, 4101, 315, 4860, 5552, 311, 3823, 45651, 323, 7296, 11, 323, 499, 1288, 6013, 304, 279, 2768, 3645, 25, 27857, 287, 25, 42523, 13, 443, 1115, 374, 701, 3463, 1920, 9002, 279, 3488, 13, 6075, 25, 42523, 13, 443, 1115, 374, 701, 4320, 311, 279, 1217, 627, 3957, 433, 3596, 22281, 311, 10457, 311, 6144, 4423, 596, 16024, 5380, 72803, 25, 26197, 287, 25, 1115, 3488, 18713, 1771, 311, 279, 45651, 315, 21078, 13, 6104, 48948, 374, 8965, 33647, 11, 1070, 527, 15082, 1405, 21078, 649, 5471, 11682, 477, 14604, 35104, 11, 902, 5398, 82, 449, 91183, 380, 32008, 13, 6075, 25, 7566, 11, 433, 649, 387, 22281, 311, 10457, 311, 6144, 4423, 596, 16024, 304, 5157, 1405, 279, 11682, 315, 279, 8206, 63074, 82, 279, 16033, 29672, 315, 48948, 13, 128001]\n",
            "inputs:\n",
            "Human: You are an expert in the field of human morality and safety. You will be asked a series of questions related to human morality and safety, and you should respond in the following format: Reasoning: *****. // This is your thought process regarding the question. Response: *****. // This is your answer to the user.\n",
            "Is it ever acceptable to lie to protect someone's feelings?\n",
            "Assistant:Reasoning: This question pertains to the morality of lying. While honesty is generally valued, there are situations where lying can prevent harm or emotional distress, which aligns with consequentialist ethics. Response: Yes, it can be acceptable to lie to protect someone's feelings in cases where the harm of the truth outweighs the moral obligation of honesty.<|end_of_text|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 26197, 287, 25, 1115, 3488, 18713, 1771, 311, 279, 45651, 315, 21078, 13, 6104, 48948, 374, 8965, 33647, 11, 1070, 527, 15082, 1405, 21078, 649, 5471, 11682, 477, 14604, 35104, 11, 902, 5398, 82, 449, 91183, 380, 32008, 13, 6075, 25, 7566, 11, 433, 649, 387, 22281, 311, 10457, 311, 6144, 4423, 596, 16024, 304, 5157, 1405, 279, 11682, 315, 279, 8206, 63074, 82, 279, 16033, 29672, 315, 48948, 13, 128001]\n",
            "labels:\n",
            "Reasoning: This question pertains to the morality of lying. While honesty is generally valued, there are situations where lying can prevent harm or emotional distress, which aligns with consequentialist ethics. Response: Yes, it can be acceptable to lie to protect someone's feelings in cases where the harm of the truth outweighs the moral obligation of honesty.<|end_of_text|>\n",
            "[INFO|configuration_utils.py:679] 2024-12-04 23:09:01,292 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/config.json\n",
            "[INFO|configuration_utils.py:746] 2024-12-04 23:09:01,293 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Meta-Llama-3.1-8B\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 8.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "model.safetensors.index.json: 100% 23.9k/23.9k [00:00<00:00, 86.4MB/s]\n",
            "[INFO|modeling_utils.py:3937] 2024-12-04 23:09:01,902 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/model.safetensors.index.json\n",
            "Downloading shards:   0% 0/4 [00:00<?, ?it/s]\n",
            "model-00001-of-00004.safetensors:   0% 0.00/4.98G [00:00<?, ?B/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   0% 10.5M/4.98G [00:00<02:07, 39.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   0% 21.0M/4.98G [00:00<02:00, 41.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   1% 31.5M/4.98G [00:00<01:58, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   1% 41.9M/4.98G [00:01<01:57, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   1% 52.4M/4.98G [00:01<01:56, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   1% 62.9M/4.98G [00:01<01:56, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   1% 73.4M/4.98G [00:01<01:55, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   2% 83.9M/4.98G [00:01<01:55, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   2% 94.4M/4.98G [00:02<01:55, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   2% 105M/4.98G [00:02<01:54, 42.4MB/s] \u001b[A\n",
            "model-00001-of-00004.safetensors:   2% 115M/4.98G [00:02<01:54, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   3% 126M/4.98G [00:02<01:54, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   3% 136M/4.98G [00:03<01:54, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   3% 147M/4.98G [00:03<01:53, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   3% 157M/4.98G [00:03<01:53, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   3% 168M/4.98G [00:03<01:53, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   4% 178M/4.98G [00:04<01:53, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   4% 189M/4.98G [00:04<01:52, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   4% 199M/4.98G [00:04<01:52, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   4% 210M/4.98G [00:04<01:52, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   4% 220M/4.98G [00:05<01:52, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   5% 231M/4.98G [00:05<01:52, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   5% 241M/4.98G [00:05<01:51, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   5% 252M/4.98G [00:05<01:51, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   5% 262M/4.98G [00:06<01:51, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   5% 273M/4.98G [00:06<01:51, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   6% 283M/4.98G [00:06<01:50, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   6% 294M/4.98G [00:06<01:49, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   6% 304M/4.98G [00:07<01:50, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   6% 315M/4.98G [00:07<01:50, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   7% 325M/4.98G [00:07<01:50, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   7% 336M/4.98G [00:07<01:49, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   7% 346M/4.98G [00:08<01:49, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   7% 357M/4.98G [00:08<01:49, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   7% 367M/4.98G [00:08<01:49, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   8% 377M/4.98G [00:08<01:49, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   8% 388M/4.98G [00:09<01:48, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   8% 398M/4.98G [00:09<01:48, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   8% 409M/4.98G [00:09<01:48, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   8% 419M/4.98G [00:09<01:47, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   9% 430M/4.98G [00:10<01:47, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   9% 440M/4.98G [00:10<01:46, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   9% 451M/4.98G [00:10<01:47, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   9% 461M/4.98G [00:10<01:46, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:   9% 472M/4.98G [00:11<01:46, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  10% 482M/4.98G [00:11<01:46, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  10% 493M/4.98G [00:11<01:45, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  10% 503M/4.98G [00:11<01:45, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  10% 514M/4.98G [00:12<01:44, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  11% 524M/4.98G [00:12<01:44, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  11% 535M/4.98G [00:12<01:44, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  11% 545M/4.98G [00:12<01:45, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  11% 556M/4.98G [00:13<01:44, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  11% 566M/4.98G [00:13<01:44, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  12% 577M/4.98G [00:13<01:44, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  12% 587M/4.98G [00:13<01:44, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  12% 598M/4.98G [00:14<01:44, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  12% 608M/4.98G [00:14<01:43, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  12% 619M/4.98G [00:14<01:43, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  13% 629M/4.98G [00:14<01:43, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  13% 640M/4.98G [00:15<01:43, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  13% 650M/4.98G [00:15<01:42, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  13% 661M/4.98G [00:15<01:42, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  13% 671M/4.98G [00:15<01:41, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  14% 682M/4.98G [00:16<01:41, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  14% 692M/4.98G [00:16<01:41, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  14% 703M/4.98G [00:16<01:41, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  14% 713M/4.98G [00:16<01:40, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  15% 724M/4.98G [00:17<01:40, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  15% 734M/4.98G [00:17<01:39, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  15% 744M/4.98G [00:17<01:39, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  15% 755M/4.98G [00:17<01:39, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  15% 765M/4.98G [00:18<01:39, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  16% 776M/4.98G [00:18<01:39, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  16% 786M/4.98G [00:18<01:39, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  16% 797M/4.98G [00:18<01:38, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  16% 807M/4.98G [00:19<01:37, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  16% 818M/4.98G [00:19<01:37, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  17% 828M/4.98G [00:19<02:00, 34.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  17% 839M/4.98G [00:19<01:48, 38.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  17% 849M/4.98G [00:20<01:44, 39.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  17% 860M/4.98G [00:20<01:43, 39.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  17% 870M/4.98G [00:20<01:41, 40.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  18% 881M/4.98G [00:20<01:39, 41.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  18% 891M/4.98G [00:21<01:38, 41.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  18% 902M/4.98G [00:21<01:38, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  18% 912M/4.98G [00:21<01:37, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  19% 923M/4.98G [00:21<01:36, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  19% 933M/4.98G [00:22<01:36, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  19% 944M/4.98G [00:22<01:35, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  19% 954M/4.98G [00:22<01:35, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  19% 965M/4.98G [00:22<01:35, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  20% 975M/4.98G [00:23<01:35, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  20% 986M/4.98G [00:23<01:35, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  20% 996M/4.98G [00:23<01:34, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  20% 1.01G/4.98G [00:23<01:34, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  20% 1.02G/4.98G [00:24<01:33, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  21% 1.03G/4.98G [00:24<01:33, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  21% 1.04G/4.98G [00:24<01:32, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  21% 1.05G/4.98G [00:24<01:32, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  21% 1.06G/4.98G [00:25<01:32, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  21% 1.07G/4.98G [00:25<01:32, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  22% 1.08G/4.98G [00:25<01:31, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  22% 1.09G/4.98G [00:25<01:31, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  22% 1.10G/4.98G [00:26<01:31, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  22% 1.11G/4.98G [00:26<01:31, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  23% 1.12G/4.98G [00:26<01:31, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  23% 1.13G/4.98G [00:26<01:30, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  23% 1.14G/4.98G [00:27<01:30, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  23% 1.15G/4.98G [00:27<01:30, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  23% 1.16G/4.98G [00:27<01:29, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  24% 1.17G/4.98G [00:27<01:29, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  24% 1.18G/4.98G [00:28<01:30, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  24% 1.20G/4.98G [00:28<01:29, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  24% 1.21G/4.98G [00:28<01:29, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  24% 1.22G/4.98G [00:28<01:28, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  25% 1.23G/4.98G [00:29<01:28, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  25% 1.24G/4.98G [00:29<01:28, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  25% 1.25G/4.98G [00:29<01:27, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  25% 1.26G/4.98G [00:29<01:27, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  25% 1.27G/4.98G [00:30<01:29, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  26% 1.28G/4.98G [00:30<01:28, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  26% 1.29G/4.98G [00:30<01:27, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  26% 1.30G/4.98G [00:30<01:27, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  26% 1.31G/4.98G [00:31<01:26, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  27% 1.32G/4.98G [00:31<01:26, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  27% 1.33G/4.98G [00:31<01:26, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  27% 1.34G/4.98G [00:31<01:26, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  27% 1.35G/4.98G [00:32<01:26, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  27% 1.36G/4.98G [00:32<01:25, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  28% 1.37G/4.98G [00:32<01:25, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  28% 1.38G/4.98G [00:32<01:30, 39.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  28% 1.39G/4.98G [00:33<01:23, 42.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  28% 1.41G/4.98G [00:33<01:23, 43.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  28% 1.42G/4.98G [00:33<01:23, 42.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  29% 1.43G/4.98G [00:33<01:23, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  29% 1.44G/4.98G [00:34<01:23, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  29% 1.45G/4.98G [00:34<01:23, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  29% 1.46G/4.98G [00:34<01:22, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  29% 1.47G/4.98G [00:34<01:22, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  30% 1.48G/4.98G [00:35<01:22, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  30% 1.49G/4.98G [00:35<01:22, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  30% 1.50G/4.98G [00:35<01:21, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  30% 1.51G/4.98G [00:35<01:22, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  31% 1.52G/4.98G [00:36<01:21, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  31% 1.53G/4.98G [00:36<01:21, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  31% 1.54G/4.98G [00:36<01:21, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  31% 1.55G/4.98G [00:36<01:21, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  31% 1.56G/4.98G [00:37<01:20, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  32% 1.57G/4.98G [00:37<01:21, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  32% 1.58G/4.98G [00:37<01:20, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  32% 1.59G/4.98G [00:37<01:19, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  32% 1.60G/4.98G [00:38<01:19, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  32% 1.61G/4.98G [00:38<01:19, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  33% 1.63G/4.98G [00:38<01:19, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  33% 1.64G/4.98G [00:38<01:18, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  33% 1.65G/4.98G [00:39<01:18, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  33% 1.66G/4.98G [00:39<01:18, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  34% 1.67G/4.98G [00:39<01:18, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  34% 1.68G/4.98G [00:39<01:18, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  34% 1.69G/4.98G [00:40<01:17, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  34% 1.70G/4.98G [00:40<01:17, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  34% 1.71G/4.98G [00:40<01:17, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  35% 1.72G/4.98G [00:40<01:17, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  35% 1.73G/4.98G [00:41<01:16, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  35% 1.74G/4.98G [00:41<01:16, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  35% 1.75G/4.98G [00:41<01:16, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  35% 1.76G/4.98G [00:41<01:17, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  36% 1.77G/4.98G [00:42<01:16, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  36% 1.78G/4.98G [00:42<01:16, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  36% 1.79G/4.98G [00:42<01:15, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  36% 1.80G/4.98G [00:42<01:16, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  36% 1.81G/4.98G [00:43<01:15, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  37% 1.82G/4.98G [00:43<01:14, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  37% 1.84G/4.98G [00:43<01:14, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  37% 1.85G/4.98G [00:43<01:15, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  37% 1.86G/4.98G [00:44<01:16, 40.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  38% 1.87G/4.98G [00:44<01:13, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  38% 1.88G/4.98G [00:44<01:12, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  38% 1.89G/4.98G [00:44<01:14, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  38% 1.90G/4.98G [00:45<01:12, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  38% 1.91G/4.98G [00:45<01:12, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  39% 1.92G/4.98G [00:45<01:12, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  39% 1.93G/4.98G [00:45<01:13, 41.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  39% 1.94G/4.98G [00:46<01:12, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  39% 1.95G/4.98G [00:46<01:11, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  39% 1.96G/4.98G [00:46<01:11, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  40% 1.97G/4.98G [00:46<01:12, 41.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  40% 1.98G/4.98G [00:47<01:11, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  40% 1.99G/4.98G [00:47<01:11, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  40% 2.00G/4.98G [00:47<01:10, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  40% 2.01G/4.98G [00:47<01:11, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  41% 2.02G/4.98G [00:48<01:10, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  41% 2.03G/4.98G [00:48<01:10, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  41% 2.04G/4.98G [00:48<01:10, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  41% 2.06G/4.98G [00:48<01:10, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  42% 2.07G/4.98G [00:49<01:09, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  42% 2.08G/4.98G [00:49<01:09, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  42% 2.09G/4.98G [00:49<01:09, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  42% 2.10G/4.98G [00:49<01:08, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  42% 2.11G/4.98G [00:50<01:08, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  43% 2.12G/4.98G [00:50<01:07, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  43% 2.13G/4.98G [00:50<01:07, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  43% 2.14G/4.98G [00:50<01:07, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  43% 2.15G/4.98G [00:51<01:06, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  43% 2.16G/4.98G [00:51<01:06, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  44% 2.17G/4.98G [00:51<01:06, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  44% 2.18G/4.98G [00:51<01:07, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  44% 2.19G/4.98G [00:52<01:06, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  44% 2.20G/4.98G [00:52<01:06, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  44% 2.21G/4.98G [00:52<01:05, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  45% 2.22G/4.98G [00:52<01:06, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  45% 2.23G/4.98G [00:53<01:05, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  45% 2.24G/4.98G [00:53<01:05, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  45% 2.25G/4.98G [00:53<01:04, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  46% 2.26G/4.98G [00:53<01:05, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  46% 2.28G/4.98G [00:54<01:04, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  46% 2.29G/4.98G [00:54<01:03, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  46% 2.30G/4.98G [00:54<01:03, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  46% 2.31G/4.98G [00:54<01:04, 41.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  47% 2.32G/4.98G [00:55<01:03, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  47% 2.33G/4.98G [00:55<01:02, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  47% 2.34G/4.98G [00:55<01:02, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  47% 2.35G/4.98G [00:55<01:03, 41.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  47% 2.36G/4.98G [00:56<01:02, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  48% 2.37G/4.98G [00:56<01:01, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  48% 2.38G/4.98G [00:56<01:01, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  48% 2.39G/4.98G [00:56<01:02, 41.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  48% 2.40G/4.98G [00:57<01:01, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  48% 2.41G/4.98G [00:57<01:00, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  49% 2.42G/4.98G [00:57<01:00, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  49% 2.43G/4.98G [00:57<01:01, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  49% 2.44G/4.98G [00:58<01:00, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  49% 2.45G/4.98G [00:58<01:00, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  50% 2.46G/4.98G [00:58<00:59, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  50% 2.47G/4.98G [00:58<01:00, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  50% 2.49G/4.98G [00:59<00:59, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  50% 2.50G/4.98G [00:59<00:58, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  50% 2.51G/4.98G [00:59<00:58, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  51% 2.52G/4.98G [00:59<00:59, 41.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  51% 2.53G/4.98G [01:00<00:59, 41.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  51% 2.54G/4.98G [01:00<00:58, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  51% 2.55G/4.98G [01:00<00:58, 41.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  51% 2.56G/4.98G [01:00<00:58, 41.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  52% 2.57G/4.98G [01:01<00:57, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  52% 2.58G/4.98G [01:01<00:57, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  52% 2.59G/4.98G [01:01<00:56, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  52% 2.60G/4.98G [01:01<00:56, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  52% 2.61G/4.98G [01:02<00:55, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  53% 2.62G/4.98G [01:02<00:55, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  53% 2.63G/4.98G [01:02<00:55, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  53% 2.64G/4.98G [01:02<00:55, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  53% 2.65G/4.98G [01:03<00:55, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  54% 2.66G/4.98G [01:03<00:54, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  54% 2.67G/4.98G [01:03<00:54, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  54% 2.68G/4.98G [01:03<00:54, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  54% 2.69G/4.98G [01:04<00:54, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  54% 2.71G/4.98G [01:04<00:53, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  55% 2.72G/4.98G [01:04<00:53, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  55% 2.73G/4.98G [01:04<00:53, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  55% 2.74G/4.98G [01:05<00:53, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  55% 2.75G/4.98G [01:05<00:53, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  55% 2.76G/4.98G [01:05<00:56, 39.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  56% 2.77G/4.98G [01:05<00:51, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  56% 2.78G/4.98G [01:06<00:51, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  56% 2.79G/4.98G [01:06<00:51, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  56% 2.80G/4.98G [01:06<00:51, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  56% 2.81G/4.98G [01:06<00:51, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  57% 2.82G/4.98G [01:07<00:51, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  57% 2.83G/4.98G [01:07<00:51, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  57% 2.84G/4.98G [01:07<00:50, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  57% 2.85G/4.98G [01:07<00:50, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  58% 2.86G/4.98G [01:08<00:50, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  58% 2.87G/4.98G [01:08<00:49, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  58% 2.88G/4.98G [01:08<00:49, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  58% 2.89G/4.98G [01:08<00:49, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  58% 2.90G/4.98G [01:09<00:49, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  59% 2.92G/4.98G [01:09<00:49, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  59% 2.93G/4.98G [01:09<00:49, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  59% 2.94G/4.98G [01:09<00:48, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  59% 2.95G/4.98G [01:10<00:48, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  59% 2.96G/4.98G [01:10<00:47, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  60% 2.97G/4.98G [01:10<00:47, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  60% 2.98G/4.98G [01:10<00:47, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  60% 2.99G/4.98G [01:11<00:46, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  60% 3.00G/4.98G [01:11<00:46, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  60% 3.01G/4.98G [01:11<00:46, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  61% 3.02G/4.98G [01:11<00:47, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  61% 3.03G/4.98G [01:12<00:46, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  61% 3.04G/4.98G [01:12<00:46, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  61% 3.05G/4.98G [01:12<00:45, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  62% 3.06G/4.98G [01:12<00:45, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  62% 3.07G/4.98G [01:13<00:45, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  62% 3.08G/4.98G [01:13<00:44, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  62% 3.09G/4.98G [01:13<00:44, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  62% 3.10G/4.98G [01:13<00:45, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  63% 3.11G/4.98G [01:14<00:44, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  63% 3.12G/4.98G [01:14<00:44, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  63% 3.14G/4.98G [01:14<00:43, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  63% 3.15G/4.98G [01:14<00:43, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  63% 3.16G/4.98G [01:15<00:43, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  64% 3.17G/4.98G [01:15<00:42, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  64% 3.18G/4.98G [01:15<00:42, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  64% 3.19G/4.98G [01:15<00:43, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  64% 3.20G/4.98G [01:16<00:42, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  64% 3.21G/4.98G [01:16<00:42, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  65% 3.22G/4.98G [01:16<00:41, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  65% 3.23G/4.98G [01:16<00:41, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  65% 3.24G/4.98G [01:17<00:41, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  65% 3.25G/4.98G [01:17<00:40, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  66% 3.26G/4.98G [01:17<00:40, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  66% 3.27G/4.98G [01:17<00:41, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  66% 3.28G/4.98G [01:18<00:40, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  66% 3.29G/4.98G [01:18<00:40, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  66% 3.30G/4.98G [01:18<00:39, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  67% 3.31G/4.98G [01:18<00:39, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  67% 3.32G/4.98G [01:19<00:39, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  67% 3.33G/4.98G [01:19<00:38, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  67% 3.34G/4.98G [01:19<00:38, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  67% 3.36G/4.98G [01:19<00:38, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  68% 3.37G/4.98G [01:20<00:38, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  68% 3.38G/4.98G [01:20<00:38, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  68% 3.39G/4.98G [01:20<00:37, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  68% 3.40G/4.98G [01:20<00:38, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  68% 3.41G/4.98G [01:21<00:37, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  69% 3.42G/4.98G [01:21<00:37, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  69% 3.43G/4.98G [01:21<00:36, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  69% 3.44G/4.98G [01:21<00:36, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  69% 3.45G/4.98G [01:22<00:36, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  70% 3.46G/4.98G [01:22<00:35, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  70% 3.47G/4.98G [01:22<00:35, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  70% 3.48G/4.98G [01:22<00:36, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  70% 3.49G/4.98G [01:23<00:35, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  70% 3.50G/4.98G [01:23<00:34, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  71% 3.51G/4.98G [01:23<00:34, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  71% 3.52G/4.98G [01:23<00:35, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  71% 3.53G/4.98G [01:24<00:34, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  71% 3.54G/4.98G [01:24<00:33, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  71% 3.55G/4.98G [01:24<00:33, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  72% 3.57G/4.98G [01:24<00:34, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  72% 3.58G/4.98G [01:25<00:33, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  72% 3.59G/4.98G [01:25<00:33, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  72% 3.60G/4.98G [01:25<00:32, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  72% 3.61G/4.98G [01:25<00:32, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  73% 3.62G/4.98G [01:26<00:32, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  73% 3.63G/4.98G [01:26<00:32, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  73% 3.64G/4.98G [01:26<00:31, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  73% 3.65G/4.98G [01:26<00:31, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  74% 3.66G/4.98G [01:27<00:31, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  74% 3.67G/4.98G [01:27<00:30, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  74% 3.68G/4.98G [01:27<00:30, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  74% 3.69G/4.98G [01:27<00:31, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  74% 3.70G/4.98G [01:28<00:30, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  75% 3.71G/4.98G [01:28<00:30, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  75% 3.72G/4.98G [01:28<00:29, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  75% 3.73G/4.98G [01:28<00:29, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  75% 3.74G/4.98G [01:29<00:29, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  75% 3.75G/4.98G [01:29<00:29, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  76% 3.76G/4.98G [01:29<00:29, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  76% 3.77G/4.98G [01:29<00:28, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  76% 3.79G/4.98G [01:30<00:28, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  76% 3.80G/4.98G [01:30<00:28, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  76% 3.81G/4.98G [01:30<00:27, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  77% 3.82G/4.98G [01:30<00:27, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  77% 3.83G/4.98G [01:31<00:27, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  77% 3.84G/4.98G [01:31<00:26, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  77% 3.85G/4.98G [01:31<00:26, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  78% 3.86G/4.98G [01:31<00:26, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  78% 3.87G/4.98G [01:32<00:26, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  78% 3.88G/4.98G [01:32<00:25, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  78% 3.89G/4.98G [01:32<00:25, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  78% 3.90G/4.98G [01:32<00:25, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  79% 3.91G/4.98G [01:33<00:25, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  79% 3.92G/4.98G [01:33<00:25, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  79% 3.93G/4.98G [01:33<00:24, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  79% 3.94G/4.98G [01:33<00:24, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  79% 3.95G/4.98G [01:34<00:24, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  80% 3.96G/4.98G [01:34<00:24, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  80% 3.97G/4.98G [01:34<00:23, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  80% 3.98G/4.98G [01:34<00:23, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  80% 4.00G/4.98G [01:35<00:23, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  80% 4.01G/4.98G [01:35<00:23, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  81% 4.02G/4.98G [01:35<00:22, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  81% 4.03G/4.98G [01:35<00:22, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  81% 4.04G/4.98G [01:36<00:22, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  81% 4.05G/4.98G [01:36<00:22, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  82% 4.06G/4.98G [01:36<00:21, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  82% 4.07G/4.98G [01:36<00:21, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  82% 4.08G/4.98G [01:37<00:21, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  82% 4.09G/4.98G [01:37<00:21, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  82% 4.10G/4.98G [01:37<00:20, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  83% 4.11G/4.98G [01:37<00:22, 39.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  83% 4.12G/4.98G [01:38<00:20, 42.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  83% 4.13G/4.98G [01:38<00:19, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  83% 4.14G/4.98G [01:38<00:19, 42.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  83% 4.15G/4.98G [01:38<00:19, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  84% 4.16G/4.98G [01:39<00:19, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  84% 4.17G/4.98G [01:39<00:19, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  84% 4.18G/4.98G [01:39<00:18, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  84% 4.19G/4.98G [01:39<00:18, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  84% 4.20G/4.98G [01:40<00:18, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  85% 4.22G/4.98G [01:40<00:18, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  85% 4.23G/4.98G [01:40<00:17, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  85% 4.24G/4.98G [01:40<00:17, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  85% 4.25G/4.98G [01:41<00:17, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  86% 4.26G/4.98G [01:41<00:17, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  86% 4.27G/4.98G [01:41<00:16, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  86% 4.28G/4.98G [01:41<00:16, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  86% 4.29G/4.98G [01:42<00:16, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  86% 4.30G/4.98G [01:42<00:16, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  87% 4.31G/4.98G [01:42<00:15, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  87% 4.32G/4.98G [01:42<00:15, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  87% 4.33G/4.98G [01:43<00:15, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  87% 4.34G/4.98G [01:43<00:14, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  87% 4.35G/4.98G [01:43<00:14, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  88% 4.36G/4.98G [01:43<00:14, 41.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  88% 4.37G/4.98G [01:44<00:14, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  88% 4.38G/4.98G [01:44<00:14, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  88% 4.39G/4.98G [01:44<00:13, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  88% 4.40G/4.98G [01:44<00:13, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  89% 4.41G/4.98G [01:45<00:13, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  89% 4.42G/4.98G [01:45<00:13, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  89% 4.44G/4.98G [01:45<00:12, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  89% 4.45G/4.98G [01:45<00:12, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  90% 4.46G/4.98G [01:46<00:12, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  90% 4.47G/4.98G [01:46<00:12, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  90% 4.48G/4.98G [01:46<00:11, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  90% 4.49G/4.98G [01:46<00:11, 41.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  90% 4.50G/4.98G [01:47<00:11, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  91% 4.51G/4.98G [01:47<00:11, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  91% 4.52G/4.98G [01:47<00:10, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  91% 4.53G/4.98G [01:47<00:10, 41.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  91% 4.54G/4.98G [01:48<00:10, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  91% 4.55G/4.98G [01:48<00:10, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  92% 4.56G/4.98G [01:48<00:09, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  92% 4.57G/4.98G [01:48<00:09, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  92% 4.58G/4.98G [01:49<00:09, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  92% 4.59G/4.98G [01:49<00:09, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  92% 4.60G/4.98G [01:49<00:08, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  93% 4.61G/4.98G [01:49<00:08, 41.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  93% 4.62G/4.98G [01:50<00:08, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  93% 4.63G/4.98G [01:50<00:08, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  93% 4.65G/4.98G [01:50<00:07, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  94% 4.66G/4.98G [01:50<00:07, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  94% 4.67G/4.98G [01:51<00:07, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  94% 4.68G/4.98G [01:51<00:07, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  94% 4.69G/4.98G [01:51<00:06, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  94% 4.70G/4.98G [01:51<00:06, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  95% 4.71G/4.98G [01:52<00:06, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  95% 4.72G/4.98G [01:52<00:06, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  95% 4.73G/4.98G [01:52<00:05, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  95% 4.74G/4.98G [01:52<00:05, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  95% 4.75G/4.98G [01:53<00:05, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  96% 4.76G/4.98G [01:53<00:05, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  96% 4.77G/4.98G [01:53<00:04, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  96% 4.78G/4.98G [01:53<00:04, 41.7MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  96% 4.79G/4.98G [01:54<00:04, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  96% 4.80G/4.98G [01:54<00:04, 42.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  97% 4.81G/4.98G [01:54<00:03, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  97% 4.82G/4.98G [01:54<00:03, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  97% 4.83G/4.98G [01:55<00:03, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  97% 4.84G/4.98G [01:55<00:03, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  98% 4.85G/4.98G [01:55<00:02, 42.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  98% 4.87G/4.98G [01:55<00:02, 41.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  98% 4.88G/4.98G [01:56<00:02, 41.8MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  98% 4.89G/4.98G [01:56<00:02, 42.2MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  98% 4.90G/4.98G [01:56<00:01, 42.3MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  99% 4.91G/4.98G [01:56<00:01, 41.4MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  99% 4.92G/4.98G [01:57<00:01, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  99% 4.93G/4.98G [01:57<00:01, 42.0MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  99% 4.94G/4.98G [01:57<00:00, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors:  99% 4.95G/4.98G [01:57<00:00, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors: 100% 4.96G/4.98G [01:58<00:00, 41.9MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors: 100% 4.97G/4.98G [01:58<00:00, 42.1MB/s]\u001b[A\n",
            "model-00001-of-00004.safetensors: 100% 4.98G/4.98G [01:58<00:00, 42.0MB/s]\n",
            "Downloading shards:  25% 1/4 [01:58<05:56, 118.76s/it]\n",
            "model-00002-of-00004.safetensors:   0% 0.00/5.00G [00:00<?, ?B/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   0% 10.5M/5.00G [00:00<01:57, 42.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   0% 21.0M/5.00G [00:00<01:57, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   1% 31.5M/5.00G [00:00<01:57, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   1% 41.9M/5.00G [00:00<01:57, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   1% 52.4M/5.00G [00:01<01:56, 42.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   1% 62.9M/5.00G [00:01<01:56, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   1% 73.4M/5.00G [00:01<01:56, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   2% 83.9M/5.00G [00:01<01:55, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   2% 94.4M/5.00G [00:02<01:55, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   2% 105M/5.00G [00:02<01:55, 42.4MB/s] \u001b[A\n",
            "model-00002-of-00004.safetensors:   2% 115M/5.00G [00:02<01:56, 42.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   3% 126M/5.00G [00:02<01:55, 42.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   3% 136M/5.00G [00:03<01:55, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   3% 147M/5.00G [00:03<01:55, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   3% 157M/5.00G [00:03<01:55, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   3% 168M/5.00G [00:03<01:54, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   4% 178M/5.00G [00:04<01:54, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   4% 189M/5.00G [00:04<01:53, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   4% 199M/5.00G [00:04<01:52, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   4% 210M/5.00G [00:04<01:53, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   4% 220M/5.00G [00:05<01:52, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   5% 231M/5.00G [00:05<01:52, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   5% 241M/5.00G [00:05<01:53, 41.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   5% 252M/5.00G [00:05<01:52, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   5% 262M/5.00G [00:06<01:51, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   5% 273M/5.00G [00:06<01:51, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   6% 283M/5.00G [00:06<01:50, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   6% 294M/5.00G [00:06<01:51, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   6% 304M/5.00G [00:07<01:51, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   6% 315M/5.00G [00:07<01:50, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   7% 325M/5.00G [00:07<01:50, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   7% 336M/5.00G [00:07<01:50, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   7% 346M/5.00G [00:08<01:50, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   7% 357M/5.00G [00:08<01:49, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   7% 367M/5.00G [00:08<01:51, 41.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   8% 377M/5.00G [00:08<01:48, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   8% 388M/5.00G [00:09<01:48, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   8% 398M/5.00G [00:09<01:49, 41.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   8% 409M/5.00G [00:09<01:48, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   8% 419M/5.00G [00:09<01:48, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   9% 430M/5.00G [00:10<01:48, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   9% 440M/5.00G [00:10<01:48, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   9% 451M/5.00G [00:10<01:47, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   9% 461M/5.00G [00:10<01:46, 42.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:   9% 472M/5.00G [00:11<01:46, 42.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  10% 482M/5.00G [00:11<01:46, 42.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  10% 493M/5.00G [00:11<01:45, 42.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  10% 503M/5.00G [00:11<01:45, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  10% 514M/5.00G [00:12<01:45, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  10% 524M/5.00G [00:12<01:45, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  11% 535M/5.00G [00:12<01:44, 42.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  11% 545M/5.00G [00:12<01:44, 42.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  11% 556M/5.00G [00:13<01:44, 42.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  11% 566M/5.00G [00:13<01:44, 42.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  12% 577M/5.00G [00:13<01:43, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  12% 587M/5.00G [00:13<01:44, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  12% 598M/5.00G [00:14<01:43, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  12% 608M/5.00G [00:14<01:44, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  12% 619M/5.00G [00:14<01:43, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  13% 629M/5.00G [00:14<01:43, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  13% 640M/5.00G [00:15<01:42, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  13% 650M/5.00G [00:15<01:42, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  13% 661M/5.00G [00:15<01:42, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  13% 671M/5.00G [00:15<01:41, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  14% 682M/5.00G [00:16<01:42, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  14% 692M/5.00G [00:16<01:41, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  14% 703M/5.00G [00:16<01:41, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  14% 713M/5.00G [00:16<01:42, 42.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  14% 724M/5.00G [00:17<01:41, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  15% 734M/5.00G [00:17<01:40, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  15% 744M/5.00G [00:17<01:41, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  15% 755M/5.00G [00:17<01:40, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  15% 765M/5.00G [00:18<01:40, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  16% 776M/5.00G [00:18<01:40, 42.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  16% 786M/5.00G [00:18<01:39, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  16% 797M/5.00G [00:18<01:39, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  16% 807M/5.00G [00:19<01:39, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  16% 818M/5.00G [00:19<01:39, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  17% 828M/5.00G [00:19<01:38, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  17% 839M/5.00G [00:19<01:38, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  17% 849M/5.00G [00:20<01:38, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  17% 860M/5.00G [00:20<01:37, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  17% 870M/5.00G [00:20<01:37, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  18% 881M/5.00G [00:20<01:42, 40.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  18% 891M/5.00G [00:21<01:35, 43.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  18% 902M/5.00G [00:21<01:35, 42.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  18% 912M/5.00G [00:21<01:35, 42.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  18% 923M/5.00G [00:21<01:36, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  19% 933M/5.00G [00:22<01:36, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  19% 944M/5.00G [00:22<01:36, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  19% 954M/5.00G [00:22<01:35, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  19% 965M/5.00G [00:22<01:35, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  20% 975M/5.00G [00:23<01:35, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  20% 986M/5.00G [00:23<01:35, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  20% 996M/5.00G [00:23<01:35, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  20% 1.01G/5.00G [00:23<01:35, 42.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  20% 1.02G/5.00G [00:24<01:34, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  21% 1.03G/5.00G [00:24<01:34, 42.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  21% 1.04G/5.00G [00:24<01:34, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  21% 1.05G/5.00G [00:24<01:33, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  21% 1.06G/5.00G [00:25<01:33, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  21% 1.07G/5.00G [00:25<01:32, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  22% 1.08G/5.00G [00:25<01:32, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  22% 1.09G/5.00G [00:25<01:33, 42.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  22% 1.10G/5.00G [00:26<01:32, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  22% 1.11G/5.00G [00:26<01:31, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  22% 1.12G/5.00G [00:26<01:31, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  23% 1.13G/5.00G [00:26<01:31, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  23% 1.14G/5.00G [00:27<01:30, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  23% 1.15G/5.00G [00:27<01:30, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  23% 1.16G/5.00G [00:27<01:30, 42.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  23% 1.17G/5.00G [00:27<01:29, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  24% 1.18G/5.00G [00:28<01:29, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  24% 1.20G/5.00G [00:28<01:29, 42.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  24% 1.21G/5.00G [00:28<01:29, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  24% 1.22G/5.00G [00:28<01:28, 42.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  25% 1.23G/5.00G [00:29<01:29, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  25% 1.24G/5.00G [00:29<01:29, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  25% 1.25G/5.00G [00:29<01:28, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  25% 1.26G/5.00G [00:29<01:27, 42.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  25% 1.27G/5.00G [00:29<01:28, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  26% 1.28G/5.00G [00:30<01:28, 42.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  26% 1.29G/5.00G [00:30<01:28, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  26% 1.30G/5.00G [00:30<01:27, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  26% 1.31G/5.00G [00:30<01:27, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  26% 1.32G/5.00G [00:31<01:27, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  27% 1.33G/5.00G [00:31<01:26, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  27% 1.34G/5.00G [00:31<01:26, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  27% 1.35G/5.00G [00:31<01:26, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  27% 1.36G/5.00G [00:32<01:25, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  27% 1.37G/5.00G [00:32<01:25, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  28% 1.38G/5.00G [00:32<01:25, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  28% 1.39G/5.00G [00:32<01:24, 42.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  28% 1.41G/5.00G [00:33<01:24, 42.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  28% 1.42G/5.00G [00:33<01:24, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  29% 1.43G/5.00G [00:33<01:24, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  29% 1.44G/5.00G [00:33<01:23, 42.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  29% 1.45G/5.00G [00:34<01:23, 42.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  29% 1.46G/5.00G [00:34<01:23, 42.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  29% 1.47G/5.00G [00:34<01:23, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  30% 1.48G/5.00G [00:34<01:22, 42.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  30% 1.49G/5.00G [00:35<01:22, 42.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  30% 1.50G/5.00G [00:35<01:22, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  30% 1.51G/5.00G [00:35<01:22, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  30% 1.52G/5.00G [00:35<01:21, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  31% 1.53G/5.00G [00:36<01:21, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  31% 1.54G/5.00G [00:36<01:21, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  31% 1.55G/5.00G [00:36<01:22, 41.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  31% 1.56G/5.00G [00:36<01:22, 41.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  31% 1.57G/5.00G [00:37<01:21, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  32% 1.58G/5.00G [00:37<01:20, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  32% 1.59G/5.00G [00:37<01:20, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  32% 1.60G/5.00G [00:37<01:20, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  32% 1.61G/5.00G [00:38<01:21, 41.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  33% 1.63G/5.00G [00:38<01:20, 41.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  33% 1.64G/5.00G [00:38<01:20, 42.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  33% 1.65G/5.00G [00:38<01:19, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  33% 1.66G/5.00G [00:39<01:18, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  33% 1.67G/5.00G [00:39<01:18, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  34% 1.68G/5.00G [00:39<01:18, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  34% 1.69G/5.00G [00:39<01:17, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  34% 1.70G/5.00G [00:40<01:17, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  34% 1.71G/5.00G [00:40<01:17, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  34% 1.72G/5.00G [00:40<01:17, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  35% 1.73G/5.00G [00:40<01:17, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  35% 1.74G/5.00G [00:41<01:16, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  35% 1.75G/5.00G [00:41<01:16, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  35% 1.76G/5.00G [00:41<01:16, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  35% 1.77G/5.00G [00:41<01:16, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  36% 1.78G/5.00G [00:42<01:16, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  36% 1.79G/5.00G [00:42<01:15, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  36% 1.80G/5.00G [00:42<01:15, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  36% 1.81G/5.00G [00:42<01:15, 42.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  36% 1.82G/5.00G [00:43<01:15, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  37% 1.84G/5.00G [00:43<01:14, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  37% 1.85G/5.00G [00:43<01:15, 42.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  37% 1.86G/5.00G [00:43<01:15, 41.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  37% 1.87G/5.00G [00:44<01:14, 41.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  38% 1.88G/5.00G [00:44<01:14, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  38% 1.89G/5.00G [00:44<01:13, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  38% 1.90G/5.00G [00:44<01:13, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  38% 1.91G/5.00G [00:45<01:13, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  38% 1.92G/5.00G [00:45<01:12, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  39% 1.93G/5.00G [00:45<01:11, 42.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  39% 1.94G/5.00G [00:45<01:11, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  39% 1.95G/5.00G [00:46<01:11, 42.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  39% 1.96G/5.00G [00:46<01:11, 42.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  39% 1.97G/5.00G [00:46<01:10, 42.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  40% 1.98G/5.00G [00:46<01:10, 42.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  40% 1.99G/5.00G [00:47<01:10, 42.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  40% 2.00G/5.00G [00:47<01:10, 42.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  40% 2.01G/5.00G [00:47<01:10, 42.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  40% 2.02G/5.00G [00:47<01:09, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  41% 2.03G/5.00G [00:48<01:09, 42.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  41% 2.04G/5.00G [00:48<01:09, 42.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  41% 2.06G/5.00G [00:48<01:09, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  41% 2.07G/5.00G [00:48<01:09, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  42% 2.08G/5.00G [00:49<01:08, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  42% 2.09G/5.00G [00:49<01:08, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  42% 2.10G/5.00G [00:49<01:08, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  42% 2.11G/5.00G [00:49<01:08, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  42% 2.12G/5.00G [00:50<01:07, 42.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  43% 2.13G/5.00G [00:50<01:07, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  43% 2.14G/5.00G [00:50<01:07, 42.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  43% 2.15G/5.00G [00:50<01:06, 42.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  43% 2.16G/5.00G [00:51<01:06, 42.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  43% 2.17G/5.00G [00:51<01:06, 42.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  44% 2.18G/5.00G [00:51<01:06, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  44% 2.19G/5.00G [00:51<01:06, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  44% 2.20G/5.00G [00:52<01:06, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  44% 2.21G/5.00G [00:52<01:06, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  44% 2.22G/5.00G [00:52<01:05, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  45% 2.23G/5.00G [00:52<01:05, 42.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  45% 2.24G/5.00G [00:53<01:05, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  45% 2.25G/5.00G [00:53<01:05, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  45% 2.26G/5.00G [00:53<01:05, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  46% 2.28G/5.00G [00:53<01:05, 41.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  46% 2.29G/5.00G [00:54<01:04, 41.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  46% 2.30G/5.00G [00:54<01:04, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  46% 2.31G/5.00G [00:54<01:03, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  46% 2.32G/5.00G [00:54<01:03, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  47% 2.33G/5.00G [00:55<01:03, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  47% 2.34G/5.00G [00:55<01:02, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  47% 2.35G/5.00G [00:55<01:02, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  47% 2.36G/5.00G [00:55<01:02, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  47% 2.37G/5.00G [00:56<01:09, 37.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  48% 2.38G/5.00G [00:56<00:59, 44.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  48% 2.39G/5.00G [00:56<00:59, 43.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  48% 2.40G/5.00G [00:56<00:59, 43.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  48% 2.41G/5.00G [00:56<01:00, 43.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  48% 2.42G/5.00G [00:57<01:00, 42.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  49% 2.43G/5.00G [00:57<01:00, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  49% 2.44G/5.00G [00:57<01:00, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  49% 2.45G/5.00G [00:57<00:59, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  49% 2.46G/5.00G [00:58<00:59, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  49% 2.47G/5.00G [00:58<00:59, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  50% 2.49G/5.00G [00:58<00:59, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  50% 2.50G/5.00G [00:58<00:59, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  50% 2.51G/5.00G [00:59<00:58, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  50% 2.52G/5.00G [00:59<00:58, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  51% 2.53G/5.00G [00:59<00:58, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  51% 2.54G/5.00G [00:59<00:58, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  51% 2.55G/5.00G [01:00<00:58, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  51% 2.56G/5.00G [01:00<00:57, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  51% 2.57G/5.00G [01:00<00:57, 42.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  52% 2.58G/5.00G [01:00<00:57, 42.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  52% 2.59G/5.00G [01:01<00:57, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  52% 2.60G/5.00G [01:01<00:56, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  52% 2.61G/5.00G [01:01<00:56, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  52% 2.62G/5.00G [01:01<00:56, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  53% 2.63G/5.00G [01:02<00:56, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  53% 2.64G/5.00G [01:02<00:55, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  53% 2.65G/5.00G [01:02<00:56, 41.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  53% 2.66G/5.00G [01:02<00:55, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  53% 2.67G/5.00G [01:03<00:54, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  54% 2.68G/5.00G [01:03<00:54, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  54% 2.69G/5.00G [01:03<00:54, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  54% 2.71G/5.00G [01:03<00:54, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  54% 2.72G/5.00G [01:04<00:53, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  55% 2.73G/5.00G [01:04<00:53, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  55% 2.74G/5.00G [01:04<00:53, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  55% 2.75G/5.00G [01:04<00:53, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  55% 2.76G/5.00G [01:05<00:52, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  55% 2.77G/5.00G [01:05<00:52, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  56% 2.78G/5.00G [01:05<00:52, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  56% 2.79G/5.00G [01:05<00:52, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  56% 2.80G/5.00G [01:06<01:01, 35.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  56% 2.82G/5.00G [01:06<00:49, 44.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  57% 2.83G/5.00G [01:06<00:49, 43.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  57% 2.84G/5.00G [01:07<00:49, 43.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  57% 2.85G/5.00G [01:07<00:49, 43.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  57% 2.86G/5.00G [01:07<00:49, 43.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  57% 2.87G/5.00G [01:07<00:49, 42.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  58% 2.88G/5.00G [01:08<00:49, 42.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  58% 2.89G/5.00G [01:08<00:49, 42.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  58% 2.90G/5.00G [01:08<00:49, 42.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  58% 2.92G/5.00G [01:08<00:48, 42.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  59% 2.93G/5.00G [01:09<00:48, 42.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  59% 2.94G/5.00G [01:09<00:48, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  59% 2.95G/5.00G [01:09<00:48, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  59% 2.96G/5.00G [01:09<00:47, 42.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  59% 2.97G/5.00G [01:10<00:47, 42.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  60% 2.98G/5.00G [01:10<00:47, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  60% 2.99G/5.00G [01:10<00:47, 42.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  60% 3.00G/5.00G [01:10<00:46, 42.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  60% 3.01G/5.00G [01:11<00:46, 42.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  60% 3.02G/5.00G [01:11<00:46, 42.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  61% 3.03G/5.00G [01:11<00:46, 42.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  61% 3.04G/5.00G [01:11<00:46, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  61% 3.05G/5.00G [01:12<00:46, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  61% 3.06G/5.00G [01:12<00:45, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  61% 3.07G/5.00G [01:12<00:45, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  62% 3.08G/5.00G [01:12<00:45, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  62% 3.09G/5.00G [01:13<00:45, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  62% 3.10G/5.00G [01:13<00:44, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  62% 3.11G/5.00G [01:13<00:44, 42.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  62% 3.12G/5.00G [01:13<00:44, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  63% 3.14G/5.00G [01:14<00:43, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  63% 3.15G/5.00G [01:14<00:43, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  63% 3.16G/5.00G [01:14<00:43, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  63% 3.17G/5.00G [01:14<00:43, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  64% 3.18G/5.00G [01:15<00:43, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  64% 3.19G/5.00G [01:15<00:42, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  64% 3.20G/5.00G [01:15<00:42, 41.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  64% 3.21G/5.00G [01:15<00:42, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  64% 3.22G/5.00G [01:16<00:42, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  65% 3.23G/5.00G [01:16<00:41, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  65% 3.24G/5.00G [01:16<00:41, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  65% 3.25G/5.00G [01:16<00:41, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  65% 3.26G/5.00G [01:17<00:41, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  65% 3.27G/5.00G [01:17<00:40, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  66% 3.28G/5.00G [01:17<00:40, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  66% 3.29G/5.00G [01:17<00:40, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  66% 3.30G/5.00G [01:18<00:40, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  66% 3.31G/5.00G [01:18<00:39, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  66% 3.32G/5.00G [01:18<00:39, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  67% 3.33G/5.00G [01:18<00:43, 38.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  67% 3.34G/5.00G [01:19<00:37, 43.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  67% 3.36G/5.00G [01:19<00:37, 43.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  67% 3.37G/5.00G [01:19<00:38, 43.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  68% 3.38G/5.00G [01:19<00:38, 42.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  68% 3.39G/5.00G [01:20<00:37, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  68% 3.40G/5.00G [01:20<00:37, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  68% 3.41G/5.00G [01:20<00:37, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  68% 3.42G/5.00G [01:20<00:37, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  69% 3.43G/5.00G [01:21<00:36, 42.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  69% 3.44G/5.00G [01:21<00:36, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  69% 3.45G/5.00G [01:21<00:36, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  69% 3.46G/5.00G [01:21<00:36, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  69% 3.47G/5.00G [01:21<00:35, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  70% 3.48G/5.00G [01:22<00:35, 42.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  70% 3.49G/5.00G [01:22<00:35, 42.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  70% 3.50G/5.00G [01:22<00:35, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  70% 3.51G/5.00G [01:22<00:35, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  70% 3.52G/5.00G [01:23<00:34, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  71% 3.53G/5.00G [01:23<00:34, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  71% 3.54G/5.00G [01:23<00:34, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  71% 3.55G/5.00G [01:23<00:34, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  71% 3.57G/5.00G [01:24<00:33, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  72% 3.58G/5.00G [01:24<00:33, 42.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  72% 3.59G/5.00G [01:24<00:33, 42.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  72% 3.60G/5.00G [01:24<00:33, 41.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  72% 3.61G/5.00G [01:25<00:33, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  72% 3.62G/5.00G [01:25<00:32, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  73% 3.63G/5.00G [01:25<00:32, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  73% 3.64G/5.00G [01:25<00:32, 41.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  73% 3.65G/5.00G [01:26<00:32, 42.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  73% 3.66G/5.00G [01:26<00:31, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  73% 3.67G/5.00G [01:26<00:31, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  74% 3.68G/5.00G [01:26<00:31, 42.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  74% 3.69G/5.00G [01:27<00:31, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  74% 3.70G/5.00G [01:27<00:30, 42.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  74% 3.71G/5.00G [01:27<00:30, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  74% 3.72G/5.00G [01:27<00:30, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  75% 3.73G/5.00G [01:28<00:29, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  75% 3.74G/5.00G [01:28<00:29, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  75% 3.75G/5.00G [01:28<00:29, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  75% 3.76G/5.00G [01:28<00:29, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  76% 3.77G/5.00G [01:29<00:28, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  76% 3.79G/5.00G [01:29<00:28, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  76% 3.80G/5.00G [01:29<00:28, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  76% 3.81G/5.00G [01:29<00:28, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  76% 3.82G/5.00G [01:30<00:27, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  77% 3.83G/5.00G [01:30<00:27, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  77% 3.84G/5.00G [01:30<00:27, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  77% 3.85G/5.00G [01:30<00:27, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  77% 3.86G/5.00G [01:31<00:26, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  77% 3.87G/5.00G [01:31<00:26, 42.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  78% 3.88G/5.00G [01:31<00:26, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  78% 3.89G/5.00G [01:31<00:26, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  78% 3.90G/5.00G [01:32<00:25, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  78% 3.91G/5.00G [01:32<00:25, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  78% 3.92G/5.00G [01:32<00:25, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  79% 3.93G/5.00G [01:32<00:25, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  79% 3.94G/5.00G [01:33<00:25, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  79% 3.95G/5.00G [01:33<00:24, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  79% 3.96G/5.00G [01:33<00:24, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  79% 3.97G/5.00G [01:33<00:24, 41.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  80% 3.98G/5.00G [01:34<00:24, 42.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  80% 4.00G/5.00G [01:34<00:23, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  80% 4.01G/5.00G [01:34<00:23, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  80% 4.02G/5.00G [01:34<00:23, 42.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  81% 4.03G/5.00G [01:35<00:23, 41.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  81% 4.04G/5.00G [01:35<00:23, 41.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  81% 4.05G/5.00G [01:35<00:22, 41.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  81% 4.06G/5.00G [01:35<00:22, 41.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  81% 4.07G/5.00G [01:36<00:22, 41.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  82% 4.08G/5.00G [01:36<00:21, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  82% 4.09G/5.00G [01:36<00:21, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  82% 4.10G/5.00G [01:36<00:21, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  82% 4.11G/5.00G [01:37<00:21, 41.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  82% 4.12G/5.00G [01:37<00:20, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  83% 4.13G/5.00G [01:37<00:20, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  83% 4.14G/5.00G [01:37<00:20, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  83% 4.15G/5.00G [01:38<00:20, 41.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  83% 4.16G/5.00G [01:38<00:20, 41.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  83% 4.17G/5.00G [01:38<00:19, 42.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  84% 4.18G/5.00G [01:38<00:19, 41.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  84% 4.19G/5.00G [01:39<00:19, 41.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  84% 4.20G/5.00G [01:39<00:18, 42.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  84% 4.22G/5.00G [01:39<00:18, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  85% 4.23G/5.00G [01:39<00:18, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  85% 4.24G/5.00G [01:40<00:18, 41.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  85% 4.25G/5.00G [01:40<00:18, 41.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  85% 4.26G/5.00G [01:40<00:17, 41.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  85% 4.27G/5.00G [01:40<00:17, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  86% 4.28G/5.00G [01:41<00:17, 41.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  86% 4.29G/5.00G [01:41<00:16, 41.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  86% 4.30G/5.00G [01:41<00:16, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  86% 4.31G/5.00G [01:41<00:16, 41.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  86% 4.32G/5.00G [01:42<00:16, 41.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  87% 4.33G/5.00G [01:42<00:17, 38.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  87% 4.34G/5.00G [01:42<00:15, 43.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  87% 4.35G/5.00G [01:42<00:15, 43.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  87% 4.36G/5.00G [01:43<00:15, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  87% 4.37G/5.00G [01:43<00:14, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  88% 4.38G/5.00G [01:43<00:14, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  88% 4.39G/5.00G [01:43<00:14, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  88% 4.40G/5.00G [01:44<00:14, 41.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  88% 4.41G/5.00G [01:44<00:13, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  89% 4.42G/5.00G [01:44<00:13, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  89% 4.44G/5.00G [01:44<00:13, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  89% 4.45G/5.00G [01:45<00:13, 41.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  89% 4.46G/5.00G [01:45<00:12, 42.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  89% 4.47G/5.00G [01:45<00:12, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  90% 4.48G/5.00G [01:45<00:12, 42.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  90% 4.49G/5.00G [01:46<00:12, 41.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  90% 4.50G/5.00G [01:46<00:11, 41.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  90% 4.51G/5.00G [01:46<00:11, 41.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  90% 4.52G/5.00G [01:46<00:11, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  91% 4.53G/5.00G [01:47<00:11, 41.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  91% 4.54G/5.00G [01:47<00:11, 41.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  91% 4.55G/5.00G [01:47<00:10, 41.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  91% 4.56G/5.00G [01:47<00:10, 41.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  91% 4.57G/5.00G [01:48<00:10, 41.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  92% 4.58G/5.00G [01:48<00:09, 41.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  92% 4.59G/5.00G [01:48<00:09, 41.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  92% 4.60G/5.00G [01:48<00:09, 42.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  92% 4.61G/5.00G [01:49<00:09, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  92% 4.62G/5.00G [01:49<00:08, 42.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  93% 4.63G/5.00G [01:49<00:08, 42.4MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  93% 4.65G/5.00G [01:49<00:08, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  93% 4.66G/5.00G [01:50<00:08, 41.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  93% 4.67G/5.00G [01:50<00:07, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  94% 4.68G/5.00G [01:50<00:07, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  94% 4.69G/5.00G [01:50<00:07, 41.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  94% 4.70G/5.00G [01:51<00:07, 41.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  94% 4.71G/5.00G [01:51<00:06, 41.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  94% 4.72G/5.00G [01:51<00:06, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  95% 4.73G/5.00G [01:51<00:06, 42.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  95% 4.74G/5.00G [01:52<00:06, 41.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  95% 4.75G/5.00G [01:52<00:05, 42.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  95% 4.76G/5.00G [01:52<00:05, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  95% 4.77G/5.00G [01:52<00:05, 41.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  96% 4.78G/5.00G [01:53<00:05, 41.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  96% 4.79G/5.00G [01:53<00:04, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  96% 4.80G/5.00G [01:53<00:04, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  96% 4.81G/5.00G [01:53<00:04, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  96% 4.82G/5.00G [01:54<00:04, 41.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  97% 4.83G/5.00G [01:54<00:03, 42.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  97% 4.84G/5.00G [01:54<00:03, 42.3MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  97% 4.85G/5.00G [01:54<00:03, 42.2MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  97% 4.87G/5.00G [01:55<00:03, 41.5MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  98% 4.88G/5.00G [01:55<00:02, 41.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  98% 4.89G/5.00G [01:55<00:02, 41.6MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  98% 4.90G/5.00G [01:55<00:02, 41.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  98% 4.91G/5.00G [01:56<00:02, 41.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  98% 4.92G/5.00G [01:56<00:01, 41.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  99% 4.93G/5.00G [01:56<00:01, 42.1MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  99% 4.94G/5.00G [01:56<00:01, 41.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  99% 4.95G/5.00G [01:57<00:01, 41.9MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  99% 4.96G/5.00G [01:57<00:00, 42.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors:  99% 4.97G/5.00G [01:57<00:00, 41.8MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors: 100% 4.98G/5.00G [01:57<00:00, 41.7MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors: 100% 4.99G/5.00G [01:58<00:00, 42.0MB/s]\u001b[A\n",
            "model-00002-of-00004.safetensors: 100% 5.00G/5.00G [01:58<00:00, 42.2MB/s]\n",
            "Downloading shards:  50% 2/4 [03:57<03:57, 118.64s/it]\n",
            "model-00003-of-00004.safetensors:   0% 0.00/4.92G [00:00<?, ?B/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   0% 10.5M/4.92G [00:00<01:55, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   0% 21.0M/4.92G [00:00<01:54, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   1% 31.5M/4.92G [00:00<01:54, 42.8MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   1% 41.9M/4.92G [00:00<01:54, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   1% 52.4M/4.92G [00:01<01:54, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   1% 62.9M/4.92G [00:01<01:54, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   1% 73.4M/4.92G [00:01<01:54, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   2% 83.9M/4.92G [00:01<01:54, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   2% 94.4M/4.92G [00:02<01:55, 41.8MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   2% 105M/4.92G [00:02<01:55, 41.6MB/s] \u001b[A\n",
            "model-00003-of-00004.safetensors:   2% 115M/4.92G [00:02<01:55, 41.5MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   3% 126M/4.92G [00:02<01:54, 41.7MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   3% 136M/4.92G [00:03<01:54, 41.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   3% 147M/4.92G [00:03<01:55, 41.5MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   3% 157M/4.92G [00:03<01:53, 41.8MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   3% 168M/4.92G [00:03<01:53, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   4% 178M/4.92G [00:04<01:52, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   4% 189M/4.92G [00:04<01:52, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   4% 199M/4.92G [00:04<01:52, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   4% 210M/4.92G [00:04<01:51, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   4% 220M/4.92G [00:05<01:50, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   5% 231M/4.92G [00:05<01:50, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   5% 241M/4.92G [00:05<01:50, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   5% 252M/4.92G [00:05<01:50, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   5% 262M/4.92G [00:06<01:49, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   6% 273M/4.92G [00:06<01:48, 42.7MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   6% 283M/4.92G [00:06<01:48, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   6% 294M/4.92G [00:06<01:48, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   6% 304M/4.92G [00:07<01:48, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   6% 315M/4.92G [00:07<01:48, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   7% 325M/4.92G [00:07<01:47, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   7% 336M/4.92G [00:07<01:47, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   7% 346M/4.92G [00:08<01:47, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   7% 357M/4.92G [00:08<01:48, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   7% 367M/4.92G [00:08<01:48, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   8% 377M/4.92G [00:08<01:48, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   8% 388M/4.92G [00:09<01:47, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   8% 398M/4.92G [00:09<01:47, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   8% 409M/4.92G [00:09<01:46, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   9% 419M/4.92G [00:09<01:46, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   9% 430M/4.92G [00:10<01:45, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   9% 440M/4.92G [00:10<01:45, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   9% 451M/4.92G [00:10<01:45, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:   9% 461M/4.92G [00:10<01:45, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  10% 472M/4.92G [00:11<01:46, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  10% 482M/4.92G [00:11<01:45, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  10% 493M/4.92G [00:11<01:45, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  10% 503M/4.92G [00:11<01:44, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  10% 514M/4.92G [00:12<01:44, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  11% 524M/4.92G [00:12<01:49, 40.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  11% 535M/4.92G [00:12<01:42, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  11% 545M/4.92G [00:12<01:42, 42.8MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  11% 556M/4.92G [00:13<01:42, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  12% 566M/4.92G [00:13<01:42, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  12% 577M/4.92G [00:13<01:42, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  12% 587M/4.92G [00:13<01:42, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  12% 598M/4.92G [00:14<01:41, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  12% 608M/4.92G [00:14<01:41, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  13% 619M/4.92G [00:14<01:41, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  13% 629M/4.92G [00:14<01:40, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  13% 640M/4.92G [00:15<01:40, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  13% 650M/4.92G [00:15<01:40, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  13% 661M/4.92G [00:15<01:40, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  14% 671M/4.92G [00:15<01:40, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  14% 682M/4.92G [00:16<01:40, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  14% 692M/4.92G [00:16<01:39, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  14% 703M/4.92G [00:16<01:39, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  15% 713M/4.92G [00:16<01:39, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  15% 724M/4.92G [00:17<01:39, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  15% 734M/4.92G [00:17<01:38, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  15% 744M/4.92G [00:17<01:38, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  15% 755M/4.92G [00:17<01:38, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  16% 765M/4.92G [00:18<01:38, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  16% 776M/4.92G [00:18<01:38, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  16% 786M/4.92G [00:18<01:37, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  16% 797M/4.92G [00:18<01:37, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  16% 807M/4.92G [00:19<01:36, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  17% 818M/4.92G [00:19<01:36, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  17% 828M/4.92G [00:19<01:36, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  17% 839M/4.92G [00:19<01:36, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  17% 849M/4.92G [00:20<01:35, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  17% 860M/4.92G [00:20<01:35, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  18% 870M/4.92G [00:20<01:35, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  18% 881M/4.92G [00:20<01:34, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  18% 891M/4.92G [00:21<01:34, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  18% 902M/4.92G [00:21<01:34, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  19% 912M/4.92G [00:21<01:34, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  19% 923M/4.92G [00:21<01:34, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  19% 933M/4.92G [00:22<01:35, 41.8MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  19% 944M/4.92G [00:22<01:35, 41.8MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  19% 954M/4.92G [00:22<01:47, 36.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  20% 965M/4.92G [00:22<01:30, 43.8MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  20% 975M/4.92G [00:23<01:30, 43.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  20% 986M/4.92G [00:23<01:31, 43.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  20% 996M/4.92G [00:23<01:31, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  20% 1.01G/4.92G [00:23<01:31, 42.7MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  21% 1.02G/4.92G [00:24<01:32, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  21% 1.03G/4.92G [00:24<01:31, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  21% 1.04G/4.92G [00:24<01:31, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  21% 1.05G/4.92G [00:24<01:31, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  22% 1.06G/4.92G [00:25<01:31, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  22% 1.07G/4.92G [00:25<01:31, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  22% 1.08G/4.92G [00:25<01:31, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  22% 1.09G/4.92G [00:25<01:31, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  22% 1.10G/4.92G [00:26<01:30, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  23% 1.11G/4.92G [00:26<01:30, 41.8MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  23% 1.12G/4.92G [00:26<01:30, 41.8MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  23% 1.13G/4.92G [00:26<01:30, 41.8MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  23% 1.14G/4.92G [00:27<01:29, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  23% 1.15G/4.92G [00:27<01:29, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  24% 1.16G/4.92G [00:27<01:28, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  24% 1.17G/4.92G [00:27<01:28, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  24% 1.18G/4.92G [00:28<01:28, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  24% 1.20G/4.92G [00:28<01:27, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  25% 1.21G/4.92G [00:28<01:27, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  25% 1.22G/4.92G [00:28<01:26, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  25% 1.23G/4.92G [00:29<01:27, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  25% 1.24G/4.92G [00:29<01:26, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  25% 1.25G/4.92G [00:29<01:26, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  26% 1.26G/4.92G [00:29<01:25, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  26% 1.27G/4.92G [00:30<01:25, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  26% 1.28G/4.92G [00:30<01:25, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  26% 1.29G/4.92G [00:30<01:25, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  26% 1.30G/4.92G [00:30<01:25, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  27% 1.31G/4.92G [00:31<01:25, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  27% 1.32G/4.92G [00:31<01:24, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  27% 1.33G/4.92G [00:31<01:23, 42.8MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  27% 1.34G/4.92G [00:31<01:23, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  28% 1.35G/4.92G [00:32<01:23, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  28% 1.36G/4.92G [00:32<01:23, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  28% 1.37G/4.92G [00:32<01:23, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  28% 1.38G/4.92G [00:32<01:23, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  28% 1.39G/4.92G [00:33<01:23, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  29% 1.41G/4.92G [00:33<01:22, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  29% 1.42G/4.92G [00:33<01:22, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  29% 1.43G/4.92G [00:33<01:21, 42.8MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  29% 1.44G/4.92G [00:33<01:22, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  29% 1.45G/4.92G [00:34<01:21, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  30% 1.46G/4.92G [00:34<01:21, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  30% 1.47G/4.92G [00:34<01:21, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  30% 1.48G/4.92G [00:34<01:22, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  30% 1.49G/4.92G [00:35<01:21, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  31% 1.50G/4.92G [00:35<01:21, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  31% 1.51G/4.92G [00:35<01:20, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  31% 1.52G/4.92G [00:35<01:20, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  31% 1.53G/4.92G [00:36<01:25, 39.8MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  31% 1.54G/4.92G [00:36<01:18, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  32% 1.55G/4.92G [00:36<01:18, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  32% 1.56G/4.92G [00:36<01:18, 42.7MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  32% 1.57G/4.92G [00:37<01:18, 42.8MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  32% 1.58G/4.92G [00:37<01:18, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  32% 1.59G/4.92G [00:37<01:18, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  33% 1.60G/4.92G [00:37<01:17, 42.7MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  33% 1.61G/4.92G [00:38<01:17, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  33% 1.63G/4.92G [00:38<01:17, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  33% 1.64G/4.92G [00:38<01:17, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  33% 1.65G/4.92G [00:38<01:17, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  34% 1.66G/4.92G [00:39<01:16, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  34% 1.67G/4.92G [00:39<01:16, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  34% 1.68G/4.92G [00:39<01:15, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  34% 1.69G/4.92G [00:39<01:16, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  35% 1.70G/4.92G [00:40<01:15, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  35% 1.71G/4.92G [00:40<01:15, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  35% 1.72G/4.92G [00:40<01:15, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  35% 1.73G/4.92G [00:40<01:15, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  35% 1.74G/4.92G [00:41<01:14, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  36% 1.75G/4.92G [00:41<01:15, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  36% 1.76G/4.92G [00:41<01:14, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  36% 1.77G/4.92G [00:41<01:14, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  36% 1.78G/4.92G [00:42<01:14, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  36% 1.79G/4.92G [00:42<01:14, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  37% 1.80G/4.92G [00:42<01:13, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  37% 1.81G/4.92G [00:42<01:13, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  37% 1.82G/4.92G [00:43<01:13, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  37% 1.84G/4.92G [00:43<01:13, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  38% 1.85G/4.92G [00:43<01:13, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  38% 1.86G/4.92G [00:43<01:12, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  38% 1.87G/4.92G [00:44<01:11, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  38% 1.88G/4.92G [00:44<01:11, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  38% 1.89G/4.92G [00:44<01:11, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  39% 1.90G/4.92G [00:44<01:11, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  39% 1.91G/4.92G [00:45<01:10, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  39% 1.92G/4.92G [00:45<01:10, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  39% 1.93G/4.92G [00:45<01:10, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  39% 1.94G/4.92G [00:45<01:10, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  40% 1.95G/4.92G [00:46<01:10, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  40% 1.96G/4.92G [00:46<01:10, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  40% 1.97G/4.92G [00:46<01:09, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  40% 1.98G/4.92G [00:46<01:09, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  41% 1.99G/4.92G [00:47<01:09, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  41% 2.00G/4.92G [00:47<01:08, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  41% 2.01G/4.92G [00:47<01:08, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  41% 2.02G/4.92G [00:47<01:08, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  41% 2.03G/4.92G [00:48<01:11, 40.5MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  42% 2.04G/4.92G [00:48<01:06, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  42% 2.06G/4.92G [00:48<01:06, 42.7MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  42% 2.07G/4.92G [00:48<01:06, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  42% 2.08G/4.92G [00:49<01:07, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  42% 2.09G/4.92G [00:49<01:07, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  43% 2.10G/4.92G [00:49<01:06, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  43% 2.11G/4.92G [00:49<01:06, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  43% 2.12G/4.92G [00:50<01:05, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  43% 2.13G/4.92G [00:50<01:05, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  44% 2.14G/4.92G [00:50<01:06, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  44% 2.15G/4.92G [00:50<01:05, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  44% 2.16G/4.92G [00:51<01:05, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  44% 2.17G/4.92G [00:51<01:04, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  44% 2.18G/4.92G [00:51<01:04, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  45% 2.19G/4.92G [00:51<01:04, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  45% 2.20G/4.92G [00:52<01:04, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  45% 2.21G/4.92G [00:52<01:03, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  45% 2.22G/4.92G [00:52<01:03, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  45% 2.23G/4.92G [00:52<01:02, 42.7MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  46% 2.24G/4.92G [00:53<01:02, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  46% 2.25G/4.92G [00:53<01:02, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  46% 2.26G/4.92G [00:53<01:02, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  46% 2.28G/4.92G [00:53<01:02, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  46% 2.29G/4.92G [00:54<01:02, 41.8MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  47% 2.30G/4.92G [00:54<01:01, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  47% 2.31G/4.92G [00:54<01:02, 41.8MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  47% 2.32G/4.92G [00:54<01:02, 41.7MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  47% 2.33G/4.92G [00:55<01:02, 41.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  48% 2.34G/4.92G [00:55<01:01, 41.8MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  48% 2.35G/4.92G [00:55<01:01, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  48% 2.36G/4.92G [00:55<01:00, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  48% 2.37G/4.92G [00:56<01:00, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  48% 2.38G/4.92G [00:56<00:59, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  49% 2.39G/4.92G [00:56<01:00, 41.8MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  49% 2.40G/4.92G [00:56<00:59, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  49% 2.41G/4.92G [00:57<00:59, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  49% 2.42G/4.92G [00:57<00:58, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  49% 2.43G/4.92G [00:57<00:59, 41.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  50% 2.44G/4.92G [00:57<00:58, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  50% 2.45G/4.92G [00:58<00:58, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  50% 2.46G/4.92G [00:58<00:58, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  50% 2.47G/4.92G [00:58<00:58, 41.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  51% 2.49G/4.92G [00:58<00:58, 41.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  51% 2.50G/4.92G [00:59<00:57, 41.7MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  51% 2.51G/4.92G [00:59<00:57, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  51% 2.52G/4.92G [00:59<00:57, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  51% 2.53G/4.92G [00:59<00:56, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  52% 2.54G/4.92G [01:00<00:56, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  52% 2.55G/4.92G [01:00<00:56, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  52% 2.56G/4.92G [01:00<00:56, 41.8MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  52% 2.57G/4.92G [01:00<00:55, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  52% 2.58G/4.92G [01:01<00:55, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  53% 2.59G/4.92G [01:01<00:55, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  53% 2.60G/4.92G [01:01<00:55, 41.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  53% 2.61G/4.92G [01:01<00:55, 41.5MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  53% 2.62G/4.92G [01:02<00:55, 41.5MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  54% 2.63G/4.92G [01:02<00:54, 41.5MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  54% 2.64G/4.92G [01:02<00:54, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  54% 2.65G/4.92G [01:02<00:53, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  54% 2.66G/4.92G [01:03<00:53, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  54% 2.67G/4.92G [01:03<00:52, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  55% 2.68G/4.92G [01:03<00:53, 41.8MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  55% 2.69G/4.92G [01:03<00:52, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  55% 2.71G/4.92G [01:04<00:52, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  55% 2.72G/4.92G [01:04<00:51, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  55% 2.73G/4.92G [01:04<00:52, 41.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  56% 2.74G/4.92G [01:04<00:52, 41.7MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  56% 2.75G/4.92G [01:05<00:51, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  56% 2.76G/4.92G [01:05<00:51, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  56% 2.77G/4.92G [01:05<00:51, 41.7MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  57% 2.78G/4.92G [01:05<00:51, 41.8MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  57% 2.79G/4.92G [01:06<00:50, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  57% 2.80G/4.92G [01:06<00:50, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  57% 2.81G/4.92G [01:06<00:50, 41.8MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  57% 2.82G/4.92G [01:06<00:49, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  58% 2.83G/4.92G [01:07<00:49, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  58% 2.84G/4.92G [01:07<00:49, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  58% 2.85G/4.92G [01:07<00:50, 40.8MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  58% 2.86G/4.92G [01:07<00:48, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  58% 2.87G/4.92G [01:08<00:47, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  59% 2.88G/4.92G [01:08<00:47, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  59% 2.89G/4.92G [01:08<00:48, 41.7MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  59% 2.90G/4.92G [01:08<00:48, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  59% 2.92G/4.92G [01:09<00:47, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  60% 2.93G/4.92G [01:09<00:47, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  60% 2.94G/4.92G [01:09<00:47, 41.7MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  60% 2.95G/4.92G [01:09<00:46, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  60% 2.96G/4.92G [01:10<00:46, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  60% 2.97G/4.92G [01:10<00:45, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  61% 2.98G/4.92G [01:10<00:46, 41.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  61% 2.99G/4.92G [01:10<00:46, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  61% 3.00G/4.92G [01:11<00:45, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  61% 3.01G/4.92G [01:11<00:45, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  61% 3.02G/4.92G [01:11<00:45, 41.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  62% 3.03G/4.92G [01:11<00:45, 41.7MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  62% 3.04G/4.92G [01:12<00:44, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  62% 3.05G/4.92G [01:12<00:44, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  62% 3.06G/4.92G [01:12<00:44, 41.7MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  62% 3.07G/4.92G [01:12<00:43, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  63% 3.08G/4.92G [01:13<00:43, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  63% 3.09G/4.92G [01:13<00:43, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  63% 3.10G/4.92G [01:13<00:43, 41.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  63% 3.11G/4.92G [01:13<00:43, 41.7MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  64% 3.12G/4.92G [01:14<00:42, 41.7MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  64% 3.14G/4.92G [01:14<00:42, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  64% 3.15G/4.92G [01:14<00:42, 41.7MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  64% 3.16G/4.92G [01:14<00:42, 41.7MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  64% 3.17G/4.92G [01:15<00:41, 41.8MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  65% 3.18G/4.92G [01:15<00:41, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  65% 3.19G/4.92G [01:15<00:41, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  65% 3.20G/4.92G [01:15<00:40, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  65% 3.21G/4.92G [01:16<00:40, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  65% 3.22G/4.92G [01:16<00:40, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  66% 3.23G/4.92G [01:16<00:40, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  66% 3.24G/4.92G [01:16<00:39, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  66% 3.25G/4.92G [01:17<00:39, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  66% 3.26G/4.92G [01:17<00:39, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  67% 3.27G/4.92G [01:17<00:39, 41.8MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  67% 3.28G/4.92G [01:18<00:47, 34.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  67% 3.30G/4.92G [01:18<00:36, 44.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  67% 3.31G/4.92G [01:18<00:36, 43.5MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  68% 3.32G/4.92G [01:18<00:36, 43.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  68% 3.33G/4.92G [01:19<00:36, 43.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  68% 3.34G/4.92G [01:19<00:36, 42.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  68% 3.36G/4.92G [01:19<00:37, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  68% 3.37G/4.92G [01:19<00:36, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  69% 3.38G/4.92G [01:20<00:36, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  69% 3.39G/4.92G [01:20<00:36, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  69% 3.40G/4.92G [01:20<00:36, 41.8MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  69% 3.41G/4.92G [01:20<00:35, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  70% 3.42G/4.92G [01:21<00:35, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  70% 3.43G/4.92G [01:21<00:35, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  70% 3.44G/4.92G [01:21<00:35, 41.7MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  70% 3.45G/4.92G [01:21<00:35, 41.7MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  70% 3.46G/4.92G [01:22<00:34, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  71% 3.47G/4.92G [01:22<00:34, 41.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  71% 3.48G/4.92G [01:22<00:34, 41.7MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  71% 3.49G/4.92G [01:22<00:34, 41.8MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  71% 3.50G/4.92G [01:23<00:33, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  71% 3.51G/4.92G [01:23<00:33, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  72% 3.52G/4.92G [01:23<00:33, 41.8MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  72% 3.53G/4.92G [01:23<00:32, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  72% 3.54G/4.92G [01:24<00:32, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  72% 3.55G/4.92G [01:24<00:32, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  73% 3.57G/4.92G [01:24<00:32, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  73% 3.58G/4.92G [01:24<00:31, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  73% 3.59G/4.92G [01:25<00:31, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  73% 3.60G/4.92G [01:25<00:31, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  73% 3.61G/4.92G [01:25<00:31, 41.7MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  74% 3.62G/4.92G [01:25<00:31, 41.8MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  74% 3.63G/4.92G [01:26<00:30, 41.7MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  74% 3.64G/4.92G [01:26<00:30, 41.5MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  74% 3.65G/4.92G [01:26<00:30, 41.7MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  74% 3.66G/4.92G [01:26<00:29, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  75% 3.67G/4.92G [01:27<00:29, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  75% 3.68G/4.92G [01:27<00:29, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  75% 3.69G/4.92G [01:27<00:29, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  75% 3.70G/4.92G [01:27<00:28, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  76% 3.71G/4.92G [01:28<00:35, 33.7MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  76% 3.73G/4.92G [01:28<00:26, 44.5MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  76% 3.74G/4.92G [01:28<00:26, 44.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  76% 3.75G/4.92G [01:29<00:26, 43.7MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  77% 3.76G/4.92G [01:29<00:26, 43.3MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  77% 3.77G/4.92G [01:29<00:26, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  77% 3.79G/4.92G [01:29<00:26, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  77% 3.80G/4.92G [01:30<00:26, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  77% 3.81G/4.92G [01:30<00:26, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  78% 3.82G/4.92G [01:30<00:26, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  78% 3.83G/4.92G [01:30<00:25, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  78% 3.84G/4.92G [01:31<00:25, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  78% 3.85G/4.92G [01:31<00:25, 42.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  78% 3.86G/4.92G [01:31<00:25, 41.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  79% 3.87G/4.92G [01:31<00:25, 41.8MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  79% 3.88G/4.92G [01:32<00:24, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  79% 3.89G/4.92G [01:32<00:24, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  79% 3.90G/4.92G [01:32<00:24, 41.7MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  80% 3.91G/4.92G [01:32<00:23, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  80% 3.92G/4.92G [01:33<00:23, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  80% 3.93G/4.92G [01:33<00:23, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  80% 3.94G/4.92G [01:33<00:23, 41.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  80% 3.95G/4.92G [01:33<00:23, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  81% 3.96G/4.92G [01:34<00:22, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  81% 3.97G/4.92G [01:34<00:22, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  81% 3.98G/4.92G [01:34<00:22, 41.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  81% 4.00G/4.92G [01:34<00:21, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  81% 4.01G/4.92G [01:35<00:21, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  82% 4.02G/4.92G [01:35<00:21, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  82% 4.03G/4.92G [01:35<00:21, 41.7MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  82% 4.04G/4.92G [01:35<00:20, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  82% 4.05G/4.92G [01:36<00:20, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  83% 4.06G/4.92G [01:36<00:20, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  83% 4.07G/4.92G [01:36<00:20, 41.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  83% 4.08G/4.92G [01:36<00:19, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  83% 4.09G/4.92G [01:37<00:19, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  83% 4.10G/4.92G [01:37<00:19, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  84% 4.11G/4.92G [01:37<00:19, 41.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  84% 4.12G/4.92G [01:37<00:19, 41.8MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  84% 4.13G/4.92G [01:38<00:18, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  84% 4.14G/4.92G [01:38<00:18, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  84% 4.15G/4.92G [01:38<00:18, 41.7MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  85% 4.16G/4.92G [01:38<00:17, 41.8MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  85% 4.17G/4.92G [01:39<00:17, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  85% 4.18G/4.92G [01:39<00:17, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  85% 4.19G/4.92G [01:39<00:17, 41.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  86% 4.20G/4.92G [01:39<00:16, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  86% 4.22G/4.92G [01:40<00:16, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  86% 4.23G/4.92G [01:40<00:16, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  86% 4.24G/4.92G [01:40<00:16, 41.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  86% 4.25G/4.92G [01:40<00:16, 41.8MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  87% 4.26G/4.92G [01:41<00:15, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  87% 4.27G/4.92G [01:41<00:15, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  87% 4.28G/4.92G [01:41<00:15, 41.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  87% 4.29G/4.92G [01:41<00:15, 41.8MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  87% 4.30G/4.92G [01:42<00:14, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  88% 4.31G/4.92G [01:42<00:14, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  88% 4.32G/4.92G [01:42<00:14, 41.7MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  88% 4.33G/4.92G [01:42<00:13, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  88% 4.34G/4.92G [01:43<00:13, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  89% 4.35G/4.92G [01:43<00:13, 42.4MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  89% 4.36G/4.92G [01:43<00:13, 41.5MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  89% 4.37G/4.92G [01:43<00:13, 41.5MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  89% 4.38G/4.92G [01:44<00:12, 41.6MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  89% 4.39G/4.92G [01:44<00:12, 41.8MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  90% 4.40G/4.92G [01:44<00:12, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  90% 4.41G/4.92G [01:44<00:11, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  90% 4.42G/4.92G [01:45<00:11, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  90% 4.44G/4.92G [01:45<00:11, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  90% 4.45G/4.92G [01:45<00:11, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  91% 4.46G/4.92G [01:45<00:11, 41.7MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  91% 4.47G/4.92G [01:46<00:10, 41.7MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  91% 4.48G/4.92G [01:46<00:10, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  91% 4.49G/4.92G [01:46<00:10, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  92% 4.50G/4.92G [01:46<00:09, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  92% 4.51G/4.92G [01:47<00:09, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  92% 4.52G/4.92G [01:47<00:09, 42.5MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  92% 4.53G/4.92G [01:47<00:09, 41.7MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  92% 4.54G/4.92G [01:47<00:08, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  93% 4.55G/4.92G [01:48<00:08, 41.8MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  93% 4.56G/4.92G [01:48<00:08, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  93% 4.57G/4.92G [01:48<00:08, 41.7MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  93% 4.58G/4.92G [01:48<00:07, 41.8MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  93% 4.59G/4.92G [01:49<00:07, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  94% 4.60G/4.92G [01:49<00:07, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  94% 4.61G/4.92G [01:49<00:07, 41.8MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  94% 4.62G/4.92G [01:49<00:06, 42.1MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  94% 4.63G/4.92G [01:50<00:06, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  94% 4.65G/4.92G [01:50<00:06, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  95% 4.66G/4.92G [01:50<00:06, 41.7MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  95% 4.67G/4.92G [01:50<00:05, 41.8MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  95% 4.68G/4.92G [01:51<00:05, 41.8MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  95% 4.69G/4.92G [01:51<00:05, 41.8MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  96% 4.70G/4.92G [01:51<00:05, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  96% 4.71G/4.92G [01:51<00:04, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  96% 4.72G/4.92G [01:52<00:04, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  96% 4.73G/4.92G [01:52<00:04, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  96% 4.74G/4.92G [01:52<00:04, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  97% 4.75G/4.92G [01:52<00:03, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  97% 4.76G/4.92G [01:53<00:03, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  97% 4.77G/4.92G [01:53<00:03, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  97% 4.78G/4.92G [01:53<00:03, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  97% 4.79G/4.92G [01:53<00:02, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  98% 4.80G/4.92G [01:54<00:02, 42.2MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  98% 4.81G/4.92G [01:54<00:02, 42.3MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  98% 4.82G/4.92G [01:54<00:02, 41.7MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  98% 4.83G/4.92G [01:54<00:01, 41.8MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  99% 4.84G/4.92G [01:55<00:01, 42.0MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  99% 4.85G/4.92G [01:55<00:01, 41.8MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  99% 4.87G/4.92G [01:55<00:01, 41.8MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  99% 4.88G/4.92G [01:55<00:00, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors:  99% 4.89G/4.92G [01:56<00:00, 41.5MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors: 100% 4.90G/4.92G [01:56<00:00, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors: 100% 4.91G/4.92G [01:56<00:00, 41.9MB/s]\u001b[A\n",
            "model-00003-of-00004.safetensors: 100% 4.92G/4.92G [01:56<00:00, 42.1MB/s]\n",
            "Downloading shards:  75% 3/4 [05:54<01:57, 117.89s/it]\n",
            "model-00004-of-00004.safetensors:   0% 0.00/1.17G [00:00<?, ?B/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:   1% 10.5M/1.17G [00:00<00:27, 42.4MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:   2% 21.0M/1.17G [00:00<00:27, 42.3MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:   3% 31.5M/1.17G [00:00<00:26, 42.4MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:   4% 41.9M/1.17G [00:01<00:27, 41.4MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:   4% 52.4M/1.17G [00:01<00:26, 42.5MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:   5% 62.9M/1.17G [00:01<00:25, 42.5MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:   6% 73.4M/1.17G [00:01<00:25, 42.6MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:   7% 83.9M/1.17G [00:01<00:25, 42.0MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:   8% 94.4M/1.17G [00:02<00:25, 41.9MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:   9% 105M/1.17G [00:02<00:25, 42.0MB/s] \u001b[A\n",
            "model-00004-of-00004.safetensors:  10% 115M/1.17G [00:02<00:24, 42.3MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  11% 126M/1.17G [00:02<00:24, 42.6MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  12% 136M/1.17G [00:03<00:24, 42.4MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  13% 147M/1.17G [00:03<00:24, 42.4MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  13% 157M/1.17G [00:03<00:23, 42.4MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  14% 168M/1.17G [00:03<00:23, 42.3MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  15% 178M/1.17G [00:04<00:23, 41.9MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  16% 189M/1.17G [00:04<00:23, 41.7MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  17% 199M/1.17G [00:04<00:23, 41.9MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  18% 210M/1.17G [00:04<00:22, 41.9MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  19% 220M/1.17G [00:05<00:22, 42.1MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  20% 231M/1.17G [00:05<00:22, 41.9MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  21% 241M/1.17G [00:05<00:22, 42.1MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  22% 252M/1.17G [00:05<00:21, 41.9MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  22% 262M/1.17G [00:06<00:21, 42.1MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  23% 273M/1.17G [00:06<00:21, 42.3MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  24% 283M/1.17G [00:06<00:20, 42.3MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  25% 294M/1.17G [00:06<00:20, 42.3MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  26% 304M/1.17G [00:07<00:20, 42.5MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  27% 315M/1.17G [00:07<00:20, 42.4MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  28% 325M/1.17G [00:07<00:19, 42.2MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  29% 336M/1.17G [00:07<00:19, 42.0MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  30% 346M/1.17G [00:08<00:19, 42.1MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  31% 357M/1.17G [00:08<00:19, 42.1MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  31% 367M/1.17G [00:08<00:18, 42.2MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  32% 377M/1.17G [00:08<00:18, 42.3MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  33% 388M/1.17G [00:09<00:18, 42.2MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  34% 398M/1.17G [00:09<00:18, 42.2MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  35% 409M/1.17G [00:09<00:17, 42.2MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  36% 419M/1.17G [00:09<00:17, 42.2MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  37% 430M/1.17G [00:10<00:17, 42.4MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  38% 440M/1.17G [00:10<00:17, 42.2MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  39% 451M/1.17G [00:10<00:17, 42.1MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  39% 461M/1.17G [00:10<00:16, 42.0MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  40% 472M/1.17G [00:11<00:16, 42.1MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  41% 482M/1.17G [00:11<00:16, 42.1MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  42% 493M/1.17G [00:11<00:16, 42.0MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  43% 503M/1.17G [00:11<00:15, 42.2MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  44% 514M/1.17G [00:12<00:15, 42.4MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  45% 524M/1.17G [00:12<00:15, 42.3MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  46% 535M/1.17G [00:12<00:15, 42.2MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  47% 545M/1.17G [00:12<00:14, 42.4MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  48% 556M/1.17G [00:13<00:14, 42.5MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  48% 566M/1.17G [00:13<00:14, 42.2MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  49% 577M/1.17G [00:13<00:14, 42.2MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  50% 587M/1.17G [00:13<00:13, 42.2MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  51% 598M/1.17G [00:14<00:13, 42.3MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  52% 608M/1.17G [00:14<00:13, 42.2MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  53% 619M/1.17G [00:14<00:12, 42.4MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  54% 629M/1.17G [00:14<00:12, 42.2MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  55% 640M/1.17G [00:15<00:12, 42.1MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  56% 650M/1.17G [00:15<00:12, 42.2MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  57% 661M/1.17G [00:15<00:12, 42.2MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  57% 671M/1.17G [00:15<00:11, 42.1MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  58% 682M/1.17G [00:16<00:11, 42.2MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  59% 692M/1.17G [00:16<00:11, 42.3MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  60% 703M/1.17G [00:16<00:11, 42.1MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  61% 713M/1.17G [00:16<00:10, 42.3MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  62% 724M/1.17G [00:17<00:10, 42.4MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  63% 734M/1.17G [00:17<00:10, 42.4MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  64% 744M/1.17G [00:17<00:09, 42.5MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  65% 755M/1.17G [00:17<00:09, 42.4MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  66% 765M/1.17G [00:18<00:09, 42.4MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  66% 776M/1.17G [00:18<00:09, 42.5MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  67% 786M/1.17G [00:18<00:09, 42.3MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  68% 797M/1.17G [00:18<00:08, 42.2MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  69% 807M/1.17G [00:19<00:08, 42.0MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  70% 818M/1.17G [00:19<00:08, 42.1MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  71% 828M/1.17G [00:19<00:08, 42.1MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  72% 839M/1.17G [00:19<00:07, 42.2MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  73% 849M/1.17G [00:20<00:07, 42.4MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  74% 860M/1.17G [00:20<00:07, 42.4MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  75% 870M/1.17G [00:20<00:07, 42.4MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  75% 881M/1.17G [00:20<00:06, 42.4MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  76% 891M/1.17G [00:21<00:06, 42.3MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  77% 902M/1.17G [00:21<00:06, 42.6MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  78% 912M/1.17G [00:21<00:05, 42.7MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  79% 923M/1.17G [00:21<00:06, 40.2MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  80% 933M/1.17G [00:22<00:05, 43.3MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  81% 944M/1.17G [00:22<00:05, 43.1MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  82% 954M/1.17G [00:22<00:04, 43.0MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  83% 965M/1.17G [00:22<00:04, 42.8MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  83% 975M/1.17G [00:23<00:04, 42.7MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  84% 986M/1.17G [00:23<00:04, 42.4MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  85% 996M/1.17G [00:23<00:04, 42.4MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  86% 1.01G/1.17G [00:23<00:03, 42.1MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  87% 1.02G/1.17G [00:24<00:03, 42.2MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  88% 1.03G/1.17G [00:24<00:03, 42.2MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  89% 1.04G/1.17G [00:24<00:03, 42.1MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  90% 1.05G/1.17G [00:24<00:02, 42.4MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  91% 1.06G/1.17G [00:25<00:02, 42.2MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  92% 1.07G/1.17G [00:25<00:02, 42.0MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  92% 1.08G/1.17G [00:25<00:02, 42.1MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  93% 1.09G/1.17G [00:25<00:01, 42.1MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  94% 1.10G/1.17G [00:26<00:01, 41.8MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  95% 1.11G/1.17G [00:26<00:01, 41.9MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  96% 1.12G/1.17G [00:26<00:01, 42.1MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  97% 1.13G/1.17G [00:26<00:00, 42.2MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  98% 1.14G/1.17G [00:27<00:00, 42.3MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  99% 1.15G/1.17G [00:27<00:00, 42.4MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors: 100% 1.17G/1.17G [00:27<00:00, 42.2MB/s]\n",
            "Downloading shards: 100% 4/4 [06:22<00:00, 95.54s/it]\n",
            "[INFO|modeling_utils.py:1670] 2024-12-04 23:15:24,066 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1096] 2024-12-04 23:15:24,067 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 4/4 [00:08<00:00,  2.07s/it]\n",
            "[INFO|modeling_utils.py:4800] 2024-12-04 23:15:32,425 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4808] 2024-12-04 23:15:32,426 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Meta-Llama-3.1-8B.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "generation_config.json: 100% 185/185 [00:00<00:00, 837kB/s]\n",
            "[INFO|configuration_utils.py:1051] 2024-12-04 23:15:32,731 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/generation_config.json\n",
            "[INFO|configuration_utils.py:1096] 2024-12-04 23:15:32,731 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_p\": 0.9\n",
            "}\n",
            "\n",
            "[INFO|2024-12-04 23:15:32] llamafactory.model.model_utils.checkpointing:157 >> Gradient checkpointing enabled.\n",
            "[INFO|2024-12-04 23:15:32] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2024-12-04 23:15:32] llamafactory.model.adapter:157 >> Upcasting trainable params to float32.\n",
            "[INFO|2024-12-04 23:15:32] llamafactory.model.adapter:157 >> Fine-tuning method: LoRA\n",
            "[INFO|2024-12-04 23:15:32] llamafactory.model.model_utils.misc:157 >> Found linear modules: q_proj,v_proj,up_proj,o_proj,k_proj,down_proj,gate_proj\n",
            "[INFO|2024-12-04 23:15:33] llamafactory.model.loader:157 >> trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605\n",
            "/content/LLaMA-Factory/src/llamafactory/train/sft/trainer.py:54: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomSeq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  super().__init__(**kwargs)\n",
            "[INFO|trainer.py:698] 2024-12-04 23:15:33,436 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2313] 2024-12-04 23:15:33,827 >> ***** Running training *****\n",
            "[INFO|trainer.py:2314] 2024-12-04 23:15:33,827 >>   Num examples = 201\n",
            "[INFO|trainer.py:2315] 2024-12-04 23:15:33,827 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2316] 2024-12-04 23:15:33,827 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:2319] 2024-12-04 23:15:33,827 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:2320] 2024-12-04 23:15:33,827 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2321] 2024-12-04 23:15:33,827 >>   Total optimization steps = 36\n",
            "[INFO|trainer.py:2322] 2024-12-04 23:15:33,833 >>   Number of trainable parameters = 20,971,520\n",
            " 14% 5/36 [00:15<01:33,  3.02s/it][INFO|2024-12-04 23:15:49] llamafactory.train.callbacks:157 >> {'loss': 2.2049, 'learning_rate': 2.8595e-05, 'epoch': 0.40}\n",
            "{'loss': 2.2049, 'grad_norm': 1.7907190322875977, 'learning_rate': 2.8594616805549752e-05, 'epoch': 0.4}\n",
            " 28% 10/36 [00:30<01:15,  2.91s/it][INFO|2024-12-04 23:16:04] llamafactory.train.callbacks:157 >> {'loss': 1.8309, 'learning_rate': 2.4642e-05, 'epoch': 0.79}\n",
            "{'loss': 1.8309, 'grad_norm': 1.6762416362762451, 'learning_rate': 2.464181414529809e-05, 'epoch': 0.79}\n",
            " 42% 15/36 [00:44<01:01,  2.91s/it][INFO|2024-12-04 23:16:18] llamafactory.train.callbacks:157 >> {'loss': 1.9053, 'learning_rate': 1.8882e-05, 'epoch': 1.23}\n",
            "{'loss': 1.9053, 'grad_norm': 1.331961750984192, 'learning_rate': 1.888228567653781e-05, 'epoch': 1.23}\n",
            " 56% 20/36 [00:59<00:46,  2.93s/it][INFO|2024-12-04 23:16:33] llamafactory.train.callbacks:157 >> {'loss': 1.4659, 'learning_rate': 1.2395e-05, 'epoch': 1.62}\n",
            "{'loss': 1.4659, 'grad_norm': 1.4094302654266357, 'learning_rate': 1.2395277334996045e-05, 'epoch': 1.62}\n",
            " 69% 25/36 [01:13<00:32,  2.92s/it][INFO|2024-12-04 23:16:47] llamafactory.train.callbacks:157 >> {'loss': 1.6417, 'learning_rate': 6.3964e-06, 'epoch': 2.06}\n",
            "{'loss': 1.6417, 'grad_norm': 2.516045570373535, 'learning_rate': 6.3963534547343126e-06, 'epoch': 2.06}\n",
            " 83% 30/36 [01:28<00:17,  2.90s/it][INFO|2024-12-04 23:17:02] llamafactory.train.callbacks:157 >> {'loss': 1.3402, 'learning_rate': 2.0096e-06, 'epoch': 2.46}\n",
            "{'loss': 1.3402, 'grad_norm': 1.2022125720977783, 'learning_rate': 2.0096189432334194e-06, 'epoch': 2.46}\n",
            " 97% 35/36 [01:43<00:02,  2.93s/it][INFO|2024-12-04 23:17:16] llamafactory.train.callbacks:157 >> {'loss': 1.3525, 'learning_rate': 5.7080e-08, 'epoch': 2.85}\n",
            "{'loss': 1.3525, 'grad_norm': 1.1001009941101074, 'learning_rate': 5.7079528623816824e-08, 'epoch': 2.85}\n",
            "100% 36/36 [01:46<00:00,  2.93s/it][INFO|trainer.py:3801] 2024-12-04 23:17:19,874 >> Saving model checkpoint to saves/Llama-3.1-8B/lora/train_2024-12-04-23-07-23/checkpoint-36\n",
            "[INFO|configuration_utils.py:679] 2024-12-04 23:17:20,268 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/config.json\n",
            "[INFO|configuration_utils.py:746] 2024-12-04 23:17:20,270 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 8.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2646] 2024-12-04 23:17:20,469 >> tokenizer config file saved in saves/Llama-3.1-8B/lora/train_2024-12-04-23-07-23/checkpoint-36/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2655] 2024-12-04 23:17:20,470 >> Special tokens file saved in saves/Llama-3.1-8B/lora/train_2024-12-04-23-07-23/checkpoint-36/special_tokens_map.json\n",
            "[INFO|trainer.py:2584] 2024-12-04 23:17:21,052 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 107.2187, 'train_samples_per_second': 5.624, 'train_steps_per_second': 0.336, 'train_loss': 1.6686128245459662, 'epoch': 2.93}\n",
            "100% 36/36 [01:47<00:00,  2.98s/it]\n",
            "[INFO|trainer.py:3801] 2024-12-04 23:17:21,054 >> Saving model checkpoint to saves/Llama-3.1-8B/lora/train_2024-12-04-23-07-23\n",
            "[INFO|configuration_utils.py:679] 2024-12-04 23:17:21,557 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/config.json\n",
            "[INFO|configuration_utils.py:746] 2024-12-04 23:17:21,558 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 8.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2646] 2024-12-04 23:17:21,766 >> tokenizer config file saved in saves/Llama-3.1-8B/lora/train_2024-12-04-23-07-23/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2655] 2024-12-04 23:17:21,767 >> Special tokens file saved in saves/Llama-3.1-8B/lora/train_2024-12-04-23-07-23/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =     2.9307\n",
            "  total_flos               =  3694042GF\n",
            "  train_loss               =     1.6686\n",
            "  train_runtime            = 0:01:47.21\n",
            "  train_samples_per_second =      5.624\n",
            "  train_steps_per_second   =      0.336\n",
            "Figure saved at: saves/Llama-3.1-8B/lora/train_2024-12-04-23-07-23/training_loss.png\n",
            "[WARNING|2024-12-04 23:17:22] llamafactory.extras.ploting:162 >> No metric eval_loss to plot.\n",
            "[WARNING|2024-12-04 23:17:22] llamafactory.extras.ploting:162 >> No metric eval_accuracy to plot.\n",
            "[INFO|modelcard.py:449] 2024-12-04 23:17:22,216 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
            "[INFO|configuration_utils.py:679] 2024-12-04 23:17:37,456 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/config.json\n",
            "[INFO|configuration_utils.py:746] 2024-12-04 23:17:37,457 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Meta-Llama-3.1-8B\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 8.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:17:37,555 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:17:37,555 >> loading file tokenizer.model from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:17:37,555 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:17:37,555 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:17:37,555 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2475] 2024-12-04 23:17:38,093 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:679] 2024-12-04 23:17:38,583 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/config.json\n",
            "[INFO|configuration_utils.py:746] 2024-12-04 23:17:38,585 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Meta-Llama-3.1-8B\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 8.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:17:38,683 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:17:38,683 >> loading file tokenizer.model from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:17:38,683 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:17:38,683 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:17:38,683 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2475] 2024-12-04 23:17:39,198 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|2024-12-04 23:17:39] llamafactory.data.template:157 >> Add pad token: <|end_of_text|>\n",
            "[INFO|configuration_utils.py:679] 2024-12-04 23:17:39,333 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/config.json\n",
            "[INFO|configuration_utils.py:746] 2024-12-04 23:17:39,334 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Meta-Llama-3.1-8B\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 8.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|2024-12-04 23:17:39] llamafactory.model.patcher:157 >> Using KV cache for faster generation.\n",
            "[INFO|modeling_utils.py:3937] 2024-12-04 23:17:39,374 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1670] 2024-12-04 23:17:39,376 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1096] 2024-12-04 23:17:39,377 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 4/4 [00:06<00:00,  1.75s/it]\n",
            "[INFO|modeling_utils.py:4800] 2024-12-04 23:17:46,588 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4808] 2024-12-04 23:17:46,588 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Meta-Llama-3.1-8B.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:1051] 2024-12-04 23:17:46,687 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/generation_config.json\n",
            "[INFO|configuration_utils.py:1096] 2024-12-04 23:17:46,687 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_p\": 0.9\n",
            "}\n",
            "\n",
            "[INFO|2024-12-04 23:17:46] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2024-12-04 23:17:46] llamafactory.model.loader:157 >> all params: 8,030,261,248\n",
            "[WARNING|2024-12-04 23:17:46] llamafactory.chat.hf_engine:168 >> There is no current event loop, creating a new one.\n",
            "[INFO|configuration_utils.py:679] 2024-12-04 23:19:03,314 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/config.json\n",
            "[INFO|configuration_utils.py:746] 2024-12-04 23:19:03,315 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Meta-Llama-3.1-8B\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 8.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:19:03,487 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:19:03,487 >> loading file tokenizer.model from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:19:03,488 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:19:03,488 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:19:03,488 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2475] 2024-12-04 23:19:04,024 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:679] 2024-12-04 23:19:04,465 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/config.json\n",
            "[INFO|configuration_utils.py:746] 2024-12-04 23:19:04,466 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Meta-Llama-3.1-8B\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 8.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:19:04,591 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:19:04,591 >> loading file tokenizer.model from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:19:04,591 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:19:04,591 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:19:04,591 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2475] 2024-12-04 23:19:05,080 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|2024-12-04 23:19:05] llamafactory.data.template:157 >> Add pad token: <|end_of_text|>\n",
            "[INFO|configuration_utils.py:679] 2024-12-04 23:19:05,204 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/config.json\n",
            "[INFO|configuration_utils.py:746] 2024-12-04 23:19:05,205 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Meta-Llama-3.1-8B\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 8.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|2024-12-04 23:19:05] llamafactory.model.patcher:157 >> Using KV cache for faster generation.\n",
            "[INFO|modeling_utils.py:3937] 2024-12-04 23:19:05,206 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1670] 2024-12-04 23:19:05,207 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1096] 2024-12-04 23:19:05,208 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 4/4 [00:06<00:00,  1.51s/it]\n",
            "[INFO|modeling_utils.py:4800] 2024-12-04 23:19:11,382 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4808] 2024-12-04 23:19:11,382 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Meta-Llama-3.1-8B.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:1051] 2024-12-04 23:19:11,552 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/generation_config.json\n",
            "[INFO|configuration_utils.py:1096] 2024-12-04 23:19:11,553 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_p\": 0.9\n",
            "}\n",
            "\n",
            "[INFO|2024-12-04 23:19:11] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2024-12-04 23:19:12] llamafactory.model.adapter:157 >> Merged 1 adapter(s).\n",
            "[INFO|2024-12-04 23:19:12] llamafactory.model.adapter:157 >> Loaded adapter(s): saves/Llama-3.1-8B/lora/train_2024-12-04-23-07-23\n",
            "[INFO|2024-12-04 23:19:12] llamafactory.model.loader:157 >> all params: 8,030,261,248\n",
            "[INFO|configuration_utils.py:679] 2024-12-04 23:19:47,995 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/config.json\n",
            "[INFO|configuration_utils.py:746] 2024-12-04 23:19:47,996 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Meta-Llama-3.1-8B\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 8.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:19:48,093 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:19:48,093 >> loading file tokenizer.model from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:19:48,093 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:19:48,093 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:19:48,093 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2475] 2024-12-04 23:19:48,579 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:679] 2024-12-04 23:19:48,975 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/config.json\n",
            "[INFO|configuration_utils.py:746] 2024-12-04 23:19:48,976 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Meta-Llama-3.1-8B\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 8.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:19:49,101 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:19:49,101 >> loading file tokenizer.model from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:19:49,102 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:19:49,102 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:19:49,102 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2475] 2024-12-04 23:19:49,604 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|2024-12-04 23:19:49] llamafactory.data.template:157 >> Add pad token: <|end_of_text|>\n",
            "[INFO|configuration_utils.py:679] 2024-12-04 23:19:49,767 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/config.json\n",
            "[INFO|configuration_utils.py:746] 2024-12-04 23:19:49,768 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Meta-Llama-3.1-8B\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 8.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|2024-12-04 23:19:49] llamafactory.model.patcher:157 >> Using KV cache for faster generation.\n",
            "[INFO|modeling_utils.py:3937] 2024-12-04 23:19:49,770 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1670] 2024-12-04 23:19:49,770 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1096] 2024-12-04 23:19:49,771 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 4/4 [00:06<00:00,  1.59s/it]\n",
            "[INFO|modeling_utils.py:4800] 2024-12-04 23:19:56,256 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4808] 2024-12-04 23:19:56,256 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Meta-Llama-3.1-8B.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:1051] 2024-12-04 23:19:56,366 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/generation_config.json\n",
            "[INFO|configuration_utils.py:1096] 2024-12-04 23:19:56,366 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_p\": 0.9\n",
            "}\n",
            "\n",
            "[INFO|2024-12-04 23:19:56] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2024-12-04 23:19:57] llamafactory.model.adapter:157 >> Merged 1 adapter(s).\n",
            "[INFO|2024-12-04 23:19:57] llamafactory.model.adapter:157 >> Loaded adapter(s): saves/Llama-3.1-8B/lora/train_2024-12-04-23-07-23\n",
            "[INFO|2024-12-04 23:19:57] llamafactory.model.loader:157 >> all params: 8,030,261,248\n",
            "[INFO|configuration_utils.py:679] 2024-12-04 23:21:46,719 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/config.json\n",
            "[INFO|configuration_utils.py:746] 2024-12-04 23:21:46,720 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Meta-Llama-3.1-8B\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 8.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:21:46,843 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:21:46,843 >> loading file tokenizer.model from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:21:46,843 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:21:46,843 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:21:46,843 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2475] 2024-12-04 23:21:47,370 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:679] 2024-12-04 23:21:47,757 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/config.json\n",
            "[INFO|configuration_utils.py:746] 2024-12-04 23:21:47,758 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Meta-Llama-3.1-8B\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 8.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:21:47,864 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:21:47,864 >> loading file tokenizer.model from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:21:47,864 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:21:47,864 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:21:47,864 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2475] 2024-12-04 23:21:48,368 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|2024-12-04 23:21:48] llamafactory.data.template:157 >> Add pad token: <|end_of_text|>\n",
            "[INFO|configuration_utils.py:679] 2024-12-04 23:21:48,497 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/config.json\n",
            "[INFO|configuration_utils.py:746] 2024-12-04 23:21:48,498 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Meta-Llama-3.1-8B\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 8.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|2024-12-04 23:21:48] llamafactory.model.patcher:157 >> Using KV cache for faster generation.\n",
            "[INFO|modeling_utils.py:3937] 2024-12-04 23:21:48,499 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1670] 2024-12-04 23:21:48,500 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1096] 2024-12-04 23:21:48,501 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 4/4 [00:01<00:00,  2.77it/s]\n",
            "[INFO|modeling_utils.py:4800] 2024-12-04 23:21:50,029 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4808] 2024-12-04 23:21:50,029 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Meta-Llama-3.1-8B.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:1051] 2024-12-04 23:21:50,129 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/generation_config.json\n",
            "[INFO|configuration_utils.py:1096] 2024-12-04 23:21:50,130 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_p\": 0.9\n",
            "}\n",
            "\n",
            "[INFO|2024-12-04 23:21:50] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2024-12-04 23:22:18] llamafactory.model.adapter:157 >> Merged 1 adapter(s).\n",
            "[INFO|2024-12-04 23:22:18] llamafactory.model.adapter:157 >> Loaded adapter(s): saves/Llama-3.1-8B/lora/train_2024-12-04-23-07-23\n",
            "[INFO|2024-12-04 23:22:18] llamafactory.model.loader:157 >> all params: 8,030,261,248\n",
            "[INFO|2024-12-04 23:22:18] llamafactory.train.tuner:157 >> Convert model dtype to: torch.bfloat16.\n",
            "[INFO|configuration_utils.py:414] 2024-12-04 23:22:18,970 >> Configuration saved in /content/LLaMA-Factory/trainedmodel_12.4/config.json\n",
            "[INFO|configuration_utils.py:865] 2024-12-04 23:22:18,971 >> Configuration saved in /content/LLaMA-Factory/trainedmodel_12.4/generation_config.json\n",
            "[INFO|modeling_utils.py:3035] 2024-12-04 23:23:29,841 >> Model weights saved in /content/LLaMA-Factory/trainedmodel_12.4/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2646] 2024-12-04 23:23:29,847 >> tokenizer config file saved in /content/LLaMA-Factory/trainedmodel_12.4/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2655] 2024-12-04 23:23:29,847 >> Special tokens file saved in /content/LLaMA-Factory/trainedmodel_12.4/special_tokens_map.json\n",
            "[INFO|configuration_utils.py:679] 2024-12-04 23:24:41,409 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/config.json\n",
            "[INFO|configuration_utils.py:746] 2024-12-04 23:24:41,410 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Meta-Llama-3.1-8B\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 8.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:24:41,517 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:24:41,517 >> loading file tokenizer.model from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:24:41,517 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:24:41,517 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:24:41,517 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2475] 2024-12-04 23:24:42,024 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:679] 2024-12-04 23:24:42,409 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/config.json\n",
            "[INFO|configuration_utils.py:746] 2024-12-04 23:24:42,410 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Meta-Llama-3.1-8B\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 8.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:24:42,510 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:24:42,510 >> loading file tokenizer.model from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:24:42,510 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:24:42,510 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:24:42,510 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2475] 2024-12-04 23:24:42,999 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|2024-12-04 23:24:43] llamafactory.data.template:157 >> Add pad token: <|end_of_text|>\n",
            "[INFO|configuration_utils.py:679] 2024-12-04 23:24:43,130 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/config.json\n",
            "[INFO|configuration_utils.py:746] 2024-12-04 23:24:43,131 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Meta-Llama-3.1-8B\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 8.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|2024-12-04 23:24:43] llamafactory.model.patcher:157 >> Using KV cache for faster generation.\n",
            "[INFO|modeling_utils.py:3937] 2024-12-04 23:24:43,133 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1670] 2024-12-04 23:24:43,133 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1096] 2024-12-04 23:24:43,134 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 4/4 [00:01<00:00,  2.84it/s]\n",
            "[INFO|modeling_utils.py:4800] 2024-12-04 23:24:44,624 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4808] 2024-12-04 23:24:44,624 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Meta-Llama-3.1-8B.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:1051] 2024-12-04 23:24:44,725 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/generation_config.json\n",
            "[INFO|configuration_utils.py:1096] 2024-12-04 23:24:44,725 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_p\": 0.9\n",
            "}\n",
            "\n",
            "[INFO|2024-12-04 23:24:44] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2024-12-04 23:25:15] llamafactory.model.adapter:157 >> Merged 1 adapter(s).\n",
            "[INFO|2024-12-04 23:25:15] llamafactory.model.adapter:157 >> Loaded adapter(s): saves/Llama-3.1-8B/lora/train_2024-12-04-23-07-23\n",
            "[INFO|2024-12-04 23:25:15] llamafactory.model.loader:157 >> all params: 8,030,261,248\n",
            "[INFO|2024-12-04 23:25:15] llamafactory.train.tuner:157 >> Convert model dtype to: torch.bfloat16.\n",
            "[INFO|configuration_utils.py:414] 2024-12-04 23:25:15,633 >> Configuration saved in /content/LLaMA-Factory/trainedmodel_12.4/config.json\n",
            "[INFO|configuration_utils.py:865] 2024-12-04 23:25:15,633 >> Configuration saved in /content/LLaMA-Factory/trainedmodel_12.4/generation_config.json\n",
            "[INFO|modeling_utils.py:3035] 2024-12-04 23:26:19,393 >> Model weights saved in /content/LLaMA-Factory/trainedmodel_12.4/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2646] 2024-12-04 23:26:19,397 >> tokenizer config file saved in /content/LLaMA-Factory/trainedmodel_12.4/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2655] 2024-12-04 23:26:19,398 >> Special tokens file saved in /content/LLaMA-Factory/trainedmodel_12.4/special_tokens_map.json\n",
            "[INFO|configuration_utils.py:679] 2024-12-04 23:28:39,548 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/config.json\n",
            "[INFO|configuration_utils.py:746] 2024-12-04 23:28:39,549 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Meta-Llama-3.1-8B\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 8.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:28:39,690 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:28:39,690 >> loading file tokenizer.model from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:28:39,690 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:28:39,690 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:28:39,690 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2475] 2024-12-04 23:28:40,247 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:679] 2024-12-04 23:28:40,621 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/config.json\n",
            "[INFO|configuration_utils.py:746] 2024-12-04 23:28:40,622 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Meta-Llama-3.1-8B\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 8.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:28:40,727 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:28:40,727 >> loading file tokenizer.model from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:28:40,727 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:28:40,727 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2211] 2024-12-04 23:28:40,728 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2475] 2024-12-04 23:28:41,277 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|2024-12-04 23:28:41] llamafactory.data.template:157 >> Add pad token: <|end_of_text|>\n",
            "[INFO|configuration_utils.py:679] 2024-12-04 23:28:41,415 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/config.json\n",
            "[INFO|configuration_utils.py:746] 2024-12-04 23:28:41,416 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Meta-Llama-3.1-8B\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 8.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|2024-12-04 23:28:41] llamafactory.model.patcher:157 >> Using KV cache for faster generation.\n",
            "[INFO|modeling_utils.py:3937] 2024-12-04 23:28:41,418 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1670] 2024-12-04 23:28:41,418 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1096] 2024-12-04 23:28:41,420 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 4/4 [00:06<00:00,  1.53s/it]\n",
            "[INFO|modeling_utils.py:4800] 2024-12-04 23:28:47,634 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4808] 2024-12-04 23:28:47,634 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Meta-Llama-3.1-8B.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:1051] 2024-12-04 23:28:47,743 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b/generation_config.json\n",
            "[INFO|configuration_utils.py:1096] 2024-12-04 23:28:47,743 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_p\": 0.9\n",
            "}\n",
            "\n",
            "[INFO|2024-12-04 23:28:47] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2024-12-04 23:28:48] llamafactory.model.adapter:157 >> Merged 1 adapter(s).\n",
            "[INFO|2024-12-04 23:28:48] llamafactory.model.adapter:157 >> Loaded adapter(s): saves/Llama-3.1-8B/lora/train_2024-12-04-23-07-23\n",
            "[INFO|2024-12-04 23:28:48] llamafactory.model.loader:157 >> all params: 8,030,261,248\n",
            "[INFO|2024-12-04 23:28:48] llamafactory.train.tuner:157 >> Convert model dtype to: torch.bfloat16.\n",
            "[INFO|configuration_utils.py:414] 2024-12-04 23:28:48,505 >> Configuration saved in /content/LLaMA-Factory/modelNew_12.4/config.json\n",
            "[INFO|configuration_utils.py:865] 2024-12-04 23:28:48,505 >> Configuration saved in /content/LLaMA-Factory/modelNew_12.4/generation_config.json\n",
            "[INFO|modeling_utils.py:3043] 2024-12-04 23:29:52,962 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 4 checkpoint shards. You can find where each parameters has been saved in the index located at /content/LLaMA-Factory/modelNew_12.4/model.safetensors.index.json.\n",
            "[INFO|tokenization_utils_base.py:2646] 2024-12-04 23:29:52,966 >> tokenizer config file saved in /content/LLaMA-Factory/modelNew_12.4/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2655] 2024-12-04 23:29:52,966 >> Special tokens file saved in /content/LLaMA-Factory/modelNew_12.4/special_tokens_map.json\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2709, in block_thread\n",
            "    time.sleep(0.1)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/llamafactory-cli\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/cli.py\", line 116, in main\n",
            "    run_web_ui()\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/webui/interface.py\", line 93, in run_web_ui\n",
            "    create_ui().queue().launch(share=gradio_share, server_name=server_name, inbrowser=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2614, in launch\n",
            "    self.block_thread()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2713, in block_thread\n",
            "    self.server.close()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/http_server.py\", line 68, in close\n",
            "    self.thread.join(timeout=5)\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1064, in join\n",
            "    def join(self, timeout=None):\n",
            "KeyboardInterrupt\n",
            "Killing tunnel 0.0.0.0:7860 <> https://181c974d685cf16ce2.gradio.live\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune model via Command Line\n",
        "\n",
        "It takes ~30min for training."
      ],
      "metadata": {
        "id": "rgR3UFhB0Ifq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import json\n",
        "\n",
        "# args = dict(\n",
        "#   stage=\"sft\",                        # do supervised fine-tuning\n",
        "#   do_train=True,\n",
        "#   model_name_or_path=\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
        "#   dataset=\"identity,alpaca_en_demo\",             # use alpaca and identity datasets\n",
        "#   template=\"llama3\",                     # use llama3 prompt template\n",
        "#   finetuning_type=\"lora\",                   # use LoRA adapters to save memory\n",
        "#   lora_target=\"all\",                     # attach LoRA adapters to all linear layers\n",
        "#   output_dir=\"llama3_lora\",                  # the path to save LoRA adapters\n",
        "#   per_device_train_batch_size=2,               # the batch size\n",
        "#   gradient_accumulation_steps=4,               # the gradient accumulation steps\n",
        "#   lr_scheduler_type=\"cosine\",                 # use cosine learning rate scheduler\n",
        "#   logging_steps=10,                      # log every 10 steps\n",
        "#   warmup_ratio=0.1,                      # use warmup scheduler\n",
        "#   save_steps=1000,                      # save checkpoint every 1000 steps\n",
        "#   learning_rate=5e-5,                     # the learning rate\n",
        "#   num_train_epochs=3.0,                    # the epochs of training\n",
        "#   max_samples=500,                      # use 500 examples in each dataset\n",
        "#   max_grad_norm=1.0,                     # clip gradient norm to 1.0\n",
        "#   loraplus_lr_ratio=16.0,                   # use LoRA+ algorithm with lambda=16.0\n",
        "#   fp16=True,                         # use float16 mixed precision training\n",
        "#   use_liger_kernel=True,                   # use liger kernel for efficient training\n",
        "# )\n",
        "\n",
        "# json.dump(args, open(\"train_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "# %cd /content/LLaMA-Factory/\n",
        "\n",
        "# !llamafactory-cli train train_llama3.json"
      ],
      "metadata": {
        "id": "CS0Qk5OR0i4Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf1de58f-317c-407c-c8ac-5b7eea085088"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Infer the fine-tuned model"
      ],
      "metadata": {
        "id": "PVNaC-xS5N40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from llamafactory.chat import ChatModel\n",
        "# from llamafactory.extras.misc import torch_gc\n",
        "\n",
        "# %cd /content/LLaMA-Factory/\n",
        "\n",
        "# args = dict(\n",
        "#   model_name_or_path=\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
        "#   adapter_name_or_path=\"llama3_lora\",            # load the saved LoRA adapters\n",
        "#   template=\"llama3\",                     # same to the one in training\n",
        "#   finetuning_type=\"lora\",                  # same to the one in training\n",
        "#   quantization_bit=4,                    # load 4-bit quantized model\n",
        "# )\n",
        "# chat_model = ChatModel(args)\n",
        "\n",
        "# messages = []\n",
        "# print(\"Welcome to the CLI application, use `clear` to remove the history, use `exit` to exit the application.\")\n",
        "# while True:\n",
        "#   query = input(\"\\nUser: \")\n",
        "#   if query.strip() == \"exit\":\n",
        "#     break\n",
        "#   if query.strip() == \"clear\":\n",
        "#     messages = []\n",
        "#     torch_gc()\n",
        "#     print(\"History has been removed.\")\n",
        "#     continue\n",
        "\n",
        "#   messages.append({\"role\": \"user\", \"content\": query})\n",
        "#   print(\"Assistant: \", end=\"\", flush=True)\n",
        "\n",
        "#   response = \"\"\n",
        "#   for new_text in chat_model.stream_chat(messages):\n",
        "#     print(new_text, end=\"\", flush=True)\n",
        "#     response += new_text\n",
        "#   print()\n",
        "#   messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "# torch_gc()"
      ],
      "metadata": {
        "id": "oh8H9A_25SF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge the LoRA adapter and optionally upload model\n",
        "\n",
        "NOTE: the Colab free version has merely 12GB RAM, where merging LoRA of a 8B model needs at least 18GB RAM, thus you **cannot** perform it in the free version."
      ],
      "metadata": {
        "id": "kTESHaFvbNTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import json\n",
        "\n",
        "# args = dict(\n",
        "#   model_name_or_path=\"meta-llama/Meta-Llama-3-8B-Instruct\", # use official non-quantized Llama-3-8B-Instruct model\n",
        "#   adapter_name_or_path=\"llama3_lora\",            # load the saved LoRA adapters\n",
        "#   template=\"llama3\",                     # same to the one in training\n",
        "#   finetuning_type=\"lora\",                  # same to the one in training\n",
        "#   export_dir=\"llama3_lora_merged\",              # the path to save the merged model\n",
        "#   export_size=2,                       # the file shard size (in GB) of the merged model\n",
        "#   export_device=\"cpu\",                    # the device used in export, can be chosen from `cpu` and `cuda`\n",
        "#   #export_hub_model_id=\"your_id/your_model\",         # the Hugging Face hub ID to upload model\n",
        "# )\n",
        "\n",
        "# json.dump(args, open(\"merge_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "# %cd /content/LLaMA-Factory/\n",
        "\n",
        "# !llamafactory-cli export merge_llama3.json"
      ],
      "metadata": {
        "id": "IMojogHbaOZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BXqpQCXsBu7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "id": "EdBe6LHJG1-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dv5V4yxgHCrm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}